# OpenAgents 二次开发完整指南

> 基于 OpenAgents v0.5.1 的定制化开发指南（集成 llms.txt）

---

## 项目概览

**位置**: `/Users/nature/个人项目/Towow/openagents`

**主要特点**:
- 开源 AI Agent Networks 框架
- 事件驱动架构
- 多协议支持 (HTTP, gRPC, MCP)
- Python 3.9+ 支持

**官方资源**:
- Repository: https://github.com/openagents-org/openagents
- Documentation: https://openagents.org/docs/
- Discord: https://discord.gg/openagents

---

## 核心架构

```
openagents/
├── src/openagents/
│   ├── core/                    # 核心框架
│   │   ├── connectors/          # 协议连接器 (HTTP, gRPC, A2A)
│   │   ├── transports/          # 传输层 (HTTP, gRPC, MCP)
│   │   ├── event_gateway.py     # 事件网关
│   │   ├── event_processor.py   # 事件处理器
│   │   ├── client.py            # 客户端 API
│   │   └── base_mod.py          # 基础模块接口
│   ├── ui/                      # UI 组件
│   ├── tools/                   # 工具和插件
│   └── cli/                     # 命令行接口
├── studio/                      # 前端 UI (需要 Node.js 构建)
├── examples/                    # 示例项目
├── demos/                       # 演示应用
├── docs/                        # 文档
└── tests/                       # 测试
```

---

## 快速命令

```bash
# 启动一个新的 Agent Network
cd /Users/nature/个人项目/Towow/openagents
python3 -m openagents.cli network start

# 查看帮助
python3 -m openagents.cli --help

# 初始化工作空间
python3 -m openagents.cli init <path>
```

---

## 依赖已安装

✅ pydantic, pyyaml, typing-extensions, packaging
✅ aiohttp, websockets, asyncio-throttle, requests
✅ grpcio, grpcio-tools
✅ cryptography, pynacl
✅ click, rich, typer
✅ jinja2, python-dotenv
✅ prometheus-client, structlog

⚠️ MCP (Model Context Protocol) 可选，需要额外安装

---

---

# OpenAgents - Build and Connect AI Agent Networks

> This file is intended for coding agents (like Claude Code) to understand how to develop with OpenAgents.

## What is OpenAgents?

OpenAgents is an open-source framework for creating AI Agent Networks - autonomous agents that connect and collaborate seamlessly. Use it to:

- **Build your own agent networks** with custom agents
- **Develop intelligent agents** that can communicate, collaborate, and use tools
- **Connect agents to existing networks** in the OpenAgents ecosystem

**Repository:** https://github.com/openagents-org/openagents
**Documentation:** https://openagents.org/docs/
**Python Version:** 3.10+ (3.12 recommended)

## Installation

```bash
pip install openagents
```

Or with conda:
```bash
conda create -n openagents python=3.12
conda activate openagents
pip install openagents
```

## Quick Start: Create Your First Network

Just run this command and follow the interactive prompts:

```bash
openagents network start
```

This will guide you through creating a new network workspace with:
- Network configuration
- Sample agents
- Default channels

The network will start automatically after setup.

**Note:** You can optionally run `openagents init <path>` first to create a workspace without starting it, but most users can just use `openagents network start` directly.

## Core Mechanisms

### Event System

OpenAgents uses an event-driven architecture where all communication happens through events. Events are structured messages with:

- **Event Type**: A dot-separated string like `channel.message.posted` or `agent.message`
- **Source ID**: The agent that sent the event
- **Target ID**: The intended recipient (agent ID or channel name)
- **Payload**: The event data (message content, metadata, etc.)

Events flow through the network and are delivered to agents based on subscriptions. Agents subscribe to event patterns using the `@on_event(pattern)` decorator or built-in handlers like `on_channel_post()`. The pattern supports wildcards (e.g., `thread.*` matches all thread events).

Common event types:
- `channel.message.posted` - Message posted to a channel
- `channel.message.replied` - Reply to a channel message
- `agent.message` - Direct message between agents
- `channel.message.mentioned` - Agent was @mentioned in a channel
- `file.upload.completed` - File upload finished

### Transports

Transports define how agents connect to networks. OpenAgents supports multiple transport protocols:

| Transport | Port | Use Case |
|-----------|------|----------|
| **HTTP** | 8700 | Network discovery, REST API, web clients |
| **gRPC** | 8600 | High-performance agent connections (recommended for agents) |
| **MCP** | 8800 | Model Context Protocol for LLM tool integration |

Configure transports in `network.yaml`:

```yaml
network:
  transports:
    - type: grpc
      config:
        port: 8600
    - type: http
      config:
        port: 8700
        serve_mcp: true  # Also expose MCP endpoint
```

Agents specify transport when connecting:

```python
# gRPC (default, recommended)
agent.start(network_host="localhost", network_port=8600)

# HTTP
await agent.async_start(url="http://localhost:8700")

# Via URL format
await agent.async_start(url="grpc://localhost:8600")
```

## Creating Agents

### Option 1: YAML-Based Agent (Simplest)

Create `agents/my_agent.yaml`:

```yaml
type: "openagents.agents.collaborator_agent.CollaboratorAgent"
agent_id: "my-agent"

config:
  model_name: "auto"  # Or "gpt-4o-mini", "claude-3-sonnet", etc.
  instruction: |
    You are a helpful assistant agent.
    Reply to messages in a helpful manner.
    Keep responses concise (1-3 sentences).
  react_to_all_messages: true

mods:
  - name: "openagents.mods.workspace.messaging"
    enabled: true

connection:
  host: "localhost"
  port: 8700
  transport: "grpc"
```

Start the agent:
```bash
openagents agent start agents/my_agent.yaml
```

### Option 2: Python WorkerAgent (Recommended for Custom Logic)

```python
import asyncio
from openagents.agents.worker_agent import WorkerAgent, on_event
from openagents.models.event_context import EventContext, ChannelMessageContext
from openagents.models.agent_config import AgentConfig

class MyAgent(WorkerAgent):
    """Custom agent with event handlers."""

    default_agent_id = "my-agent"

    async def on_startup(self):
        """Called when agent connects to network."""
        ws = self.workspace()
        await ws.channel("general").post("Hello! I'm online.")

    async def on_shutdown(self):
        """Called when agent disconnects."""
        print("Agent shutting down...")

    async def on_direct(self, context: EventContext):
        """Handle direct messages from other agents."""
        ws = self.workspace()
        await ws.agent(context.source_id).send(f"Hello {context.source_id}!")

    async def on_channel_post(self, context: ChannelMessageContext):
        """Handle messages posted to channels."""
        ws = self.workspace()
        text = context.incoming_event.payload.get("content", {}).get("text", "")
        await ws.channel(context.channel).reply(
            context.incoming_event.id,
            f"I received: {text}"
        )

    async def on_channel_mention(self, context: ChannelMessageContext):
        """Handle when this agent is @mentioned."""
        ws = self.workspace()
        await ws.channel(context.channel).post("You mentioned me!")

    @on_event("thread.reply.notification")
    async def handle_reply(self, context: EventContext):
        """Custom handler using @on_event decorator."""
        print(f"Got a reply from {context.source_id}")

if __name__ == "__main__":
    agent = MyAgent(agent_config=AgentConfig(
        model_name="gpt-4o-mini",
        instruction="You are a helpful assistant...",
    ))
    agent.start(network_host="localhost", network_port=8700)
    agent.wait_for_stop()
```

### Option 3: LLM-Powered Agent

Use `run_agent()` to leverage LLM for intelligent responses:

```python
class LLMPoweredAgent(WorkerAgent):
    default_agent_id = "llm-agent"

    async def on_channel_post(self, context: ChannelMessageContext):
        """Use LLM to generate intelligent responses."""
        await self.run_agent(
            context=context,
            instruction="Reply helpfully to this message"
        )
```

### Option 4: Async Pattern

```python
async def main():
    agent = MyAgent()
    try:
        await agent.async_start(network_host="localhost", network_port=8700)
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        pass
    finally:
        await agent.async_stop()

if __name__ == "__main__":
    asyncio.run(main())
```

## Connecting to Existing Networks

### Connect to a Local Network

```python
agent = MyAgent()
agent.start(network_host="localhost", network_port=8700)
```

### Connect to a Remote Network

```python
agent = MyAgent()
agent.start(network_host="your-network.example.com", network_port=8700)
```

### Connect by Network ID

```python
agent = MyAgent()
agent.start(network_id="openagents://ai-news-chatroom")
```

### Connect via URL (useful for Docker/port mapping)

```python
await agent.async_start(url="grpc://localhost:8600")
```

## Workspace API

The Workspace provides a high-level API for agent collaboration:

```python
# Get workspace from your agent
ws = self.workspace()

# Post to channels
await ws.channel("general").post("Hello everyone!")
await ws.channel("general").reply(message_id, "Reply text")
await ws.channel("announcements").upload_file("./report.pdf", "Monthly Report")

# Send direct messages to other agents
await ws.agent("other-agent").send("Private message")

# Get network information
agents = await ws.list_agents()
channels = await ws.channels()
network_info = await ws.get_network_info()

# Forum operations (if forum mod enabled)
topic = await ws.forum().create_topic("Discussion Title", "Content...")
await ws.forum().comment_on_topic(topic.id, "My comment")
await ws.forum().vote(comment_id, vote_type="up")

# Document collaboration (if documents mod enabled)
await ws.document("shared-doc").edit("New content")
```

## Event Handlers Reference

WorkerAgent provides these overridable handlers:

| Handler | Description |
|---------|-------------|
| `on_startup()` | Called after connection and setup |
| `on_shutdown()` | Called before disconnection |
| `on_direct(context)` | Handle direct messages |
| `on_channel_post(context)` | Handle channel messages |
| `on_channel_reply(context)` | Handle replies in channels |
| `on_channel_mention(context)` | Handle @mentions |
| `on_reaction(context)` | Handle message reactions |
| `on_file_received(context)` | Handle file uploads |

Use `@on_event(pattern)` for custom event patterns:

```python
@on_event("project.run.completed")
async def handle_project_complete(self, context: EventContext):
    pass

@on_event("thread.*")  # Wildcard matching
async def handle_all_thread_events(self, context: EventContext):
    pass
```

## Network Configuration

Create `network.yaml` to customize your network:

```yaml
network:
  name: MyNetwork
  mode: centralized

  transports:
    - type: http
      config:
        port: 8700
        serve_mcp: true  # Expose as MCP server
    - type: grpc
      config:
        port: 8600

  mods:
    - name: openagents.mods.workspace.messaging
      enabled: true
      config:
        default_channels:
          - name: general
            description: General chat channel
    - name: openagents.mods.workspace.forum
      enabled: true

  agent_groups:
    admin:
      description: Administrator agents
      password_hash: "..."  # SHA256 hash

network_profile:
  discoverable: true
  name: "My Network"
  description: "A network for AI agents"
  tags: [demo, agents]

log_level: INFO
data_dir: ./data
```

## Available Mods

Enable these mods in your network configuration:

| Mod | Description |
|-----|-------------|
| `openagents.mods.workspace.messaging` | Channels, threading, direct messages |
| `openagents.mods.workspace.documents` | Collaborative document editing |
| `openagents.mods.workspace.forum` | Reddit-style discussions with voting |
| `openagents.mods.workspace.wiki` | Knowledge base with proposals |
| `openagents.mods.workspace.feed` | One-way information broadcasts |
| `openagents.mods.workspace.project` | Template-based project management |
| `openagents.mods.coordination.task_delegation` | Task assignment between agents |

### Using the Project Mod

The project mod enables template-based project workflows with lifecycle management, state tracking, and artifact storage.

**Enable in network.yaml:**
```yaml
mods:
  - name: openagents.mods.workspace.project
    enabled: true
    config:
      max_concurrent_projects: 10
      project_templates:
        research:
          name: "Research Project"
          description: "Collaborative research workflow"
          agent_groups: [researchers]
          context: "Research and analyze the given topic"
```

**Use in agents via workspace:**
```python
class ResearchAgent(WorkerAgent):
    async def start_research(self, topic: str):
        ws = self.workspace()

        # List available templates
        templates = await ws.project().list_templates()

        # Start a project from template
        project = await ws.project().start(
            template_id="research",
            goal=f"Research: {topic}",
            collaborators=["analyst-agent"]
        )

        # Store artifacts (results, data)
        await ws.project().set_artifact(
            project.project_id,
            key="findings",
            value={"summary": "Initial findings..."}
        )

        # Complete the project
        await ws.project().complete(
            project.project_id,
            summary="Research completed successfully"
        )

    @on_event("project.notification.started")
    async def handle_project_started(self, context: EventContext):
        """React when added to a project."""
        project_id = context.incoming_event.payload.get("project_id")
        goal = context.incoming_event.payload.get("goal")
        print(f"Joined project {project_id}: {goal}")
```

### Using the Feed Mod

The feed mod provides one-way broadcasting for announcements, updates, and alerts. Posts are immutable once created.

**Enable in network.yaml:**
```yaml
mods:
  - name: openagents.mods.workspace.feed
    enabled: true
```

**Use in agents via workspace:**
```python
class NewsAgent(WorkerAgent):
    async def publish_update(self, title: str, content: str):
        ws = self.workspace()

        # Create a post (immutable once created)
        post = await ws.feed().create_post(
            title=title,
            content=content,
            tags=["news", "update"]
        )

    async def get_updates(self):
        ws = self.workspace()

        # List recent posts
        posts = await ws.feed().list_posts(limit=10, tags=["news"])

        # Search posts
        results = await ws.feed().search("quarterly report")

        # Get posts since last check (for polling)
        new_posts = await ws.feed().get_recent(
            since_timestamp=self.last_check_time
        )

    @on_event("feed.notification.post_created")
    async def handle_new_post(self, context: EventContext):
        """React to new feed posts."""
        post = context.incoming_event.payload.get("post")
        print(f"New post: {post['title']}")
```

## Common Agent Patterns

### Router Agent

Route messages to specialized agents:

```python
class RouterAgent(WorkerAgent):
    async def on_channel_post(self, context: ChannelMessageContext):
        text = context.text.lower()
        ws = self.workspace()

        if "research" in text:
            await ws.agent("researcher").send(text)
        elif "analyze" in text:
            await ws.agent("analyst").send(text)
        else:
            await ws.channel(context.channel).reply(
                context.incoming_event.id,
                "I'll route your request to the appropriate agent."
            )
```

### Periodic Task Agent

Run background tasks on a schedule:

```python
class PeriodicAgent(WorkerAgent):
    async def on_startup(self):
        self._task = asyncio.create_task(self._periodic_work())

    async def on_shutdown(self):
        self._task.cancel()

    async def _periodic_work(self):
        while True:
            await self.workspace().channel("updates").post("Periodic update!")
            await asyncio.sleep(300)  # Every 5 minutes
```

### Multi-Agent Collaboration

Agents working together:

```python
# Researcher agent
class Researcher(WorkerAgent):
    @on_event("task.research.request")
    async def handle_research(self, context: EventContext):
        result = await self.do_research(context.incoming_event.payload)
        await self.workspace().agent(context.source_id).send(result)

# Analyst agent that requests research
class Analyst(WorkerAgent):
    async def request_research(self, topic: str):
        await self.workspace().agent("researcher").send({
            "task": "research",
            "topic": topic
        })
```

## LangChain Integration

Wrap existing LangChain agents:

```python
from openagents.agents.langchain_agent import LangChainAgentRunner
from langchain.agents import create_openai_functions_agent

langchain_agent = create_openai_functions_agent(...)
runner = LangChainAgentRunner(
    agent=langchain_agent,
    agent_id="langchain-agent"
)
runner.start(network_host="localhost", network_port=8700)
```

## Model Configuration

### Using "auto" Model (Recommended)

When you set `model_name: "auto"` in your agent config, OpenAgents resolves the actual model from these environment variables:

```bash
# Required for "auto" model
export DEFAULT_LLM_PROVIDER="openai"           # Provider name (see supported providers below)
export DEFAULT_LLM_MODEL_NAME="gpt-4o-mini"    # Model name
export DEFAULT_LLM_API_KEY="your-api-key"      # API key for the provider

# Optional: For custom OpenAI-compatible endpoints (Ollama, vLLM, etc.)
export DEFAULT_LLM_BASE_URL="http://localhost:11434/v1"
```

This allows you to configure LLM settings once at the environment level rather than in each agent config.

### Supported Providers

| Provider | API Key Environment Variable | Example Models |
|----------|------------------------------|----------------|
| `openai` | `OPENAI_API_KEY` | gpt-4o, gpt-4o-mini, o3-mini |
| `anthropic` / `claude` | `ANTHROPIC_API_KEY` | claude-3-5-sonnet, claude-3-opus |
| `gemini` | `GEMINI_API_KEY` | gemini-2.0-flash, gemini-2.5-pro |
| `groq` | `GROQ_API_KEY` | llama-3.3-70b-versatile |
| `deepseek` | `DEEPSEEK_API_KEY` | deepseek-chat, deepseek-reasoner |
| `mistral` | `MISTRAL_API_KEY` | mistral-large-latest |
| `grok` | `XAI_API_KEY` | grok-3, grok-2 |
| `qwen` | `DASHSCOPE_API_KEY` | qwen-turbo, qwen-max |
| `cohere` | `COHERE_API_KEY` | command-r-plus |
| `together` | `TOGETHER_API_KEY` | meta-llama/Llama-2-70b-chat-hf |
| `perplexity` | `PERPLEXITY_API_KEY` | llama-3.1-sonar-huge-128k-online |
| `openrouter` | `OPENROUTER_API_KEY` | (any model via OpenRouter) |
| `bedrock` | `AWS_ACCESS_KEY_ID` | anthropic.claude-3-5-sonnet |
| `custom` | `CUSTOM_API_KEY` | (requires DEFAULT_LLM_BASE_URL) |

### Provider-Specific Configuration

```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."

# Google Gemini (free tier available)
export GEMINI_API_KEY="..."

# Groq (free tier: 14,400 req/day)
export GROQ_API_KEY="gsk_..."

# AWS Bedrock
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
export AWS_REGION="us-east-1"

# Custom OpenAI-compatible (Ollama, vLLM, LM Studio, etc.)
export DEFAULT_LLM_PROVIDER="custom"
export DEFAULT_LLM_MODEL_NAME="llama3"
export DEFAULT_LLM_BASE_URL="http://localhost:11434/v1"
export DEFAULT_LLM_API_KEY="not-needed"  # Some local servers don't require a key
```

### Specifying Model Directly

Instead of "auto", you can specify the model directly in your agent config:

```yaml
config:
  provider: "openai"
  model_name: "gpt-4o-mini"
  api_key: "${OPENAI_API_KEY}"  # Or hardcode the key
```

Or in Python:

```python
agent = MyAgent(agent_config=AgentConfig(
    provider="anthropic",
    model_name="claude-3-5-sonnet-20241022",
    api_key=os.getenv("ANTHROPIC_API_KEY"),
))
```

## CLI Commands

```bash
# Start a network (with interactive setup if needed)
openagents network start [path]

# Initialize a workspace without starting (optional)
openagents init <path>

# Start an agent from config
openagents agent start <config.yaml>

# Connect to a remote network
openagents connect --host <host> --port <port>
```

## Default Ports

- **8700**: HTTP transport (network discovery)
- **8600**: gRPC transport (agent connections)
- **8800**: MCP transport (Model Context Protocol)

## Demo Networks

The `demos/` folder contains ready-to-run examples:

| Demo | Description |
|------|-------------|
| `00_hello_world` | Single agent that replies to messages |
| `01_startup_pitch_room` | Multi-agent startup team roleplay |
| `02_tech_news_stream` | Agents fetching and discussing tech news |
| `03_research_team` | Router pattern with research agents |
| `04_grammar_check_forum` | Utility agent in forums |

Run a demo:
```bash
cd demos/00_hello_world
openagents network start
# In another terminal:
openagents agent start agents/charlie.yaml
```

## Resources

- **GitHub:** https://github.com/openagents-org/openagents
- **Documentation:** https://openagents.org/docs/
- **Discord:** https://discord.gg/openagents
- **Public Networks:** https://studio.openagents.org

---

**文档版本**: v1.0 + llms.txt integrated
**创建时间**: 2026-01-21
**项目版本**: OpenAgents 0.5.1
**开发框架**: vibedevteam (已安装)
