# EXP-009 实验严谨性审查

**审查日期**: 2026-02-18
**审查对象**: EXP-009 结晶协议 POC（SIM-001 + SIM-002）
**审查者立场**: 苛刻的科学审查者，首先寻找方法论缺陷

---

## 结论摘要

EXP-009 的核心声明——"v1 催化 prompt 压倒性成功，GT 发现率从 30% 提升到 80%"——存在严重的方法论缺陷，不能作为可靠的科学结论。最致命的问题是：N=1 单次运行、实验设计者同时是 GT 设计者和评估者、LLM 输出的随机性未被控制。v1 prompt 确实修复了 v0 的三个明确问题（翻译缺失、约束违反、收敛自检缺失），这些是可以置信的定性观察。但 "30% 到 80%" 这个数字传递了一种虚假的精度——它来自一个人对一次运行的主观评分，不具备任何统计意义。实验作为 prompt 迭代的探索工具是合格的；作为"验证"任何假说的证据，完全不够。

---

## 详细审查

### 1. Ground Truth 偏差

**问题严重程度：高**

这是整个实验最核心的方法论漏洞。Ground Truth（4 条预设连接）的设计者、催化 prompt 的设计者、评估者是同一个人。这构成了一个闭环的确认偏差结构：

**1.1 GT 是为这三个 Profile 量身定制的**

GT 写在 `participants.md` 和 `trigger.md` 的"张力分析"部分。这些 GT 不是从某个独立的理论框架推导出来的，而是实验设计者在创建合成 Profile 时同时设计的。换句话说：Profile 和 GT 是同一个人在同一个下午一起设计的。这意味着 Profile 中很可能有意或无意地埋入了指向这些 GT 的线索。

具体证据：

- GT1（Lina+Maya "翻译"共振）：Lina 的 Profile 有 "把虚空编织成形"、Maya 的 Profile 有 "做翻译"。这些词直接暗示了 GT 连接。
- GT3（Lina+赵维 "真实vs生成"张力）：Lina 有 "AI 生成内容和真实内容的差异"、赵维有 "隐性知识捕捉"。设计者将这两个话题放入 Profile 时，已经知道它们之间的连接。
- GT4（三人交汇：产品化）：三个 Profile 都写了"规模化/产品化"的诉求——这不是巧合，是设计。

**关键质疑**：如果换三个不是由同一人设计的真实 Profile，GT 还成立吗？GT 的"难度"是设计者主观决定的，没有任何校准。一个被设计为"应该能发现"的连接，和一个在真实场景中"有可能但不确定能发现"的连接，难度完全不同。

**1.2 GT 评分的循环论证**

v0 GT 发现率 30%，v1 GT 发现率 80%。但这些评分来自同一个人对 GT 的主观匹配。评估者已经知道 v1 做了什么修改（强化翻译），自然会更倾向于在 v1 的输出中"看到" GT 连接。

具体例子：GT3 从 v0 的 0/5 变成 v1 的 4/5。评估报告说 v1 R5-R6 出现了"如果明确 Lina 在观察，真实还存在吗？"。但原始 GT3 的定义是"隐性知识既是赵维要捕捉的数据，也是 Lina 要拍摄的内容"。R5 的"观测者效应"和原始 GT3 是同一个连接吗？评估者自己承认"质量甚至超过了预设的 GT"——这说明评估者在做的是模式匹配后的宽泛判断，而不是严格的标准对照。

GT3 拿到 4/5 说明评估标准是弹性的。如果标准是弹性的，那么 30% 和 80% 之间的差异有多少来自评估者期望的变化？

**1.3 缺少独立评估**

没有第二个评估者。没有盲评（评估者不知道哪个是 v0 哪个是 v1）。没有评估一致性检验（inter-rater reliability）。在社会科学实验中，主观评分至少需要两名独立评估者和 Cohen's Kappa 检验。这里完全缺失。

---

### 2. 样本量与可复现性

**问题严重程度：致命**

**2.1 N=1 的统计意义为零**

- 场景数量：1（东南亚制造业 CEO 品牌转型）
- 参与者组合：1（Lina + 赵维 + Maya）
- 每个配置的运行次数：1
- 总实验次数：2（SIM-001 和 SIM-002 各跑 1 次）

从统计学角度，这个实验无法支持任何定量结论。"30% 提升到 80%" 在 N=1 的条件下，等价于"张三考了 30 分，李四考了 80 分，所以李四的学习方法更好"——只有一个数据点，无法区分信号和噪音。

**2.2 LLM 输出的随机性未被控制**

两个脚本都没有设置 temperature 参数（使用 API 默认值，Anthropic API 的默认值是 1.0）。这意味着：

- SIM-001 的每一轮端侧和催化输出都带有随机性
- SIM-002 同样
- 如果用完全相同的配置再跑一次 SIM-001，输出会不同
- 如果用完全相同的配置再跑一次 SIM-002，输出也会不同

**这直接威胁到所有定量比较的有效性。**

v1 的"4 条/轮翻译"有可能在另一次运行中变成 2 条或 6 条。v0 的"0 条翻译"有可能在另一次运行中变成 1-2 条。"约束违反 ~36 次"在另一次运行中可能是 20 次或 50 次。没有多次运行的均值和标准差，所有数字都是不可靠的。

**2.3 为什么没有多种子运行？**

实验过程记录显示 SIM-001 运行约 22 分钟，SIM-002 约 23 分钟。API 成本约 $2-3/次。也就是说，重复运行 3 次的成本约 $6-9 和 45-70 分钟。这是完全可承受的。没有多次运行的唯一解释是实验设计者认为不需要——这本身就是方法论意识的缺失。

至少应该做的：

- 每个配置跑 3 次（不同 random seed 或不设 seed 自然变化）
- 报告均值和标准差
- 在 temperature=0 条件下跑一次确认性对照

---

### 3. 评估方法论

**问题严重程度：高**

**3.1 评估者 = 实验设计者 = prompt 设计者**

这是经典的观测偏差。同一个人设计了 GT、设计了 v0 prompt、观察了 v0 的失败、设计了 v1 prompt 来修复 v0 的问题、然后评估 v1 是否成功修复了那些问题。

这个人在评估 v1 时，已经知道：
- v1 做了什么修改
- v1 应该在哪些维度改善
- 什么样的输出算"成功修复"

在这种条件下，评估者不可能是客观的。

**3.2 "意外发现质量极高" 是不可证伪的主观判断**

SIM-001 评估报告说意外发现"远超预期"，SIM-002 评估报告说意外发现质量"与 v0 持平甚至更高"。但"质量极高"的标准是什么？没有定义。没有评分标准。没有独立验证。

更重要的是：LLM 产出"听起来有深度的洞察"是它的默认行为。Claude Sonnet 几乎在任何主题上都能产出"三层嵌套产品结构""观测者效应""协同即产品"这类听起来有见地的分析。这不一定代表这些洞察是有意义的——它代表的是 LLM 善于产出符合"深度分析"模板的文本。

要验证意外发现的真实价值，需要将这些发现交给领域专家（不是实验设计者）进行独立评估：这个"协同即产品"的洞察，对一个真正面临制造业转型的 CEO 来说，是有操作价值的还是听起来好听但无用的？

**3.3 约束违反的计数标准不透明**

v0 "约束违反 ~36 次"，v1 "3 次边界情况"。但：

- "约束违反"的定义是什么？评估报告列举了若干例子，但没有给出完整的违反清单和判定标准。
- "~36 次"中的"~"说明这是估算，不是精确计数。
- "边界情况"和"违反"的区分标准是什么？v1 R3 的"可能的破局点：第一次汇报不只给 CEO"为什么算"边界情况"而不是"违反"？如果同样的表述出现在 v0 中，是否会被计为"违反"？
- 有没有可能 v1 的评估者因为知道 v1 是"改进版"，而对 v1 的约束违反采用了更宽松的判定标准？

应该做的：建立一个明确的约束违反分类学（显式违反/隐式违反/边界情况），然后由两名评估者独立对所有轮次的催化输出进行编码，计算一致性。

**3.4 GT 评分使用 1-5 分制但没有量规**

GT 评分从 0/5 到 5/5，但没有给出每个分数的操作定义。什么情况是 2/5？什么情况是 3/5？4/5 和 5/5 的区别是什么？没有量规意味着评分完全依赖评估者的主观感受，不同评估者（甚至同一评估者在不同时间）可能给出不同的分数。

---

### 4. 控制变量

**问题严重程度：中等（表面看控制较好，但有隐蔽问题）**

**4.1 代码层面的变量控制**

根据对两个 `run_sim.py` 脚本的逐行比对，确认以下参数完全一致：

- 模型：都是 `claude-sonnet-4-5-20250929`
- 端侧 max_tokens：都是 2048
- 催化 max_tokens：都是 3000
- 方案生成 max_tokens：都是 4096
- temperature：都未设置（使用 API 默认值）
- 最大轮次：都是 6
- 收敛阈值：都是 2
- 端侧 prompt：完全相同的构建逻辑
- Profile 文件：完全相同
- 触发事件：SIM-002 从 SIM-001 目录加载，完全相同
- API 调用顺序：完全相同（Lina -> 赵维 -> Maya -> 催化）
- rate limiting：完全相同（0.5 秒）

唯一变量确实是催化 prompt（v0 -> v1）。从代码层面看，控制变量是干净的。这一点值得肯定。

**4.2 隐蔽的不可控变量**

尽管代码层面控制良好，但存在以下不可控变量：

1. **LLM 内部状态差异**：两次运行的时间不同（SIM-001 约 14:00，SIM-002 约 16:20）。虽然 Anthropic API 理论上不应该因时间不同产生系统性差异，但无法排除服务端负载差异导致的行为微差。

2. **端侧输出的路径依赖**：虽然端侧 prompt 模板一样，但 SIM-002 的端侧 Agent 在 R2+ 收到的催化观察（v1 格式）和 SIM-001 的端侧 Agent 收到的催化观察（v0 格式）完全不同。这意味着从 R2 开始，两个实验的整个对话历史已经分叉。v1 催化不越界给建议 -> 端侧 Agent 有更多自主空间 -> 端侧输出内容不同 -> 催化收到的输入不同 -> 催化输出进一步不同。

   这不是问题——这正是实验想要测试的因果链。但它意味着我们无法将 GT 发现率的提升 100% 归因于"v1 催化更善于发现连接"。有可能是：v1 催化不越界 -> 端侧 Agent 说了更多有用的东西 -> 催化因为输入质量更好而发现了更多连接。这是一个混杂变量。

3. **输出截断的非对称影响**：SIM-001 R1 Maya 的端侧输出在"CEO 的真实承诺"处被截断（max_tokens=2048）。SIM-002 中赵维的某些轮次也被截断。截断导致信息丢失，且丢失的内容因运行不同而不同。这是一个不受控的噪音源。

---

### 5. 主观判断依赖

**问题严重程度：高**

**5.1 "哲学洞察"和"理论贡献"作为评估正面证据**

评估报告将以下内容列为高价值发现：
- "观测者效应——如果明确 Lina 在观察，真实状态还存在吗？"
- "协同即产品——三层嵌套产品结构"
- "可诊断性概念 + 评估量表"
- "怎么开始不是程序性问题，是关系定义的时刻"

这些产出确实"听起来有深度"。但：

(a) 这些洞察是否可复现？如果用相同的 Profile 和 v1 prompt 重跑一次，会产生相同的"观测者效应"讨论吗？很可能不会。LLM 的创造性输出在不同运行间变化很大。如果这些洞察是不可复现的，那它们就不能作为 v1 prompt 优越性的证据——它们只是一次特定运行中的随机涌现。

(b) 这些洞察是 LLM 的通用能力还是催化 prompt 的功劳？给 Claude Sonnet 任何三人协作场景的对话记录，它都很可能会产出类似层次的"哲学洞察"。这不是催化 prompt 的成果，是模型的固有能力。

(c) 没有领域专家验证这些洞察的实际价值。"三层嵌套产品结构"对一个真正的商业咨询案例有用吗？"观测者效应"的讨论是不是在制造一个不存在的问题（一个真正的纪录片导演会很务实地处理这个问题，不需要哲学讨论）？

**5.2 "意外发现未退化"的判定标准**

评估报告反复声称 v1 的意外发现质量"未退化"甚至"更高"。但：

- v0 和 v1 的意外发现内容完全不同（不同的运行产生不同的涌现）
- 没有一个操作化的"意外发现质量"评分量规
- 判定"未退化"的人就是 prompt 设计者本人

严格来说，"意外发现未退化"这个结论需要：多次运行 v0 和 v1，收集意外发现，由独立评估者盲评质量，然后做统计检验。现在的证据只是一个人觉得两次运行的意外发现质量差不多。

---

### 6. 确认偏差

**问题严重程度：高**

**6.1 实验设计只能证实，不能证伪**

实验的结构是：v0 有问题 -> 针对性修复设计 v1 -> 验证 v1 是否解决了问题。这个结构天然偏向确认。

缺少的是：
- **消蚀测试（ablation）**：v1 有三个修改（翻译优先级、禁止性约束、强制格式）。哪个修改贡献了多少？没有分离。也许只有禁止性约束起作用，翻译优先级和强制格式都是无关变量。需要 v1a（只改翻译）、v1b（只改约束）、v1c（只改格式）的消蚀实验。
- **反向测试**：有没有 v1 在某个维度上变差的？评估报告说"结构分析深度从极强变为强（略减但仍丰富）"——这个"略减"被轻描淡写地处理为"可接受的交换"。但如果评估者的立场是"寻找 v1 的退化"，这个"略减"可能会被放大为一个有意义的退化信号。
- **对立假说检验**：有没有可能 v1 的 GT 发现率提升不是因为 v1 催化更好，而是因为 v1 催化的强制翻译格式恰好与 GT 的定义形式匹配？GT 的定义都是"A 和 B 之间的跨语义等价关系"——v1 催化被要求每轮输出这种格式的翻译。v0 催化没有被要求输出这种格式，所以即使它"看到了"类似的连接，也可能用结构分析的形式表达而非翻译的形式表达。GT 评分可能测量的不是"发现能力"，而是"输出格式与 GT 定义格式的匹配度"。

**6.2 叙事框架偏向成功**

评估报告的标题和结论都是庆祝性的："v1 催化 prompt 是一次成功的迭代""压倒性成功"。MEMORY.md 也使用了"压倒性成功"这个措辞。

实际发生的是：v1 在翻译输出数量和约束遵从两个维度上有明显改善（这是可信的），在 GT 发现率上有一个不可靠的数字改善，在收敛机制上未验证（因为 token 截断），在端侧同质化上未改善。

客观的叙事应该是："v1 成功修复了 v0 的三个明确缺陷（翻译缺失、约束违反、收敛格式），但由于单次运行和主观评估，改善幅度不可量化。收敛机制仍然未验证。"

**6.3 幸存者偏差：只跑了一次**

如果第一次运行的结果不好，会怎样？很可能会调整 prompt 再跑，直到出现好的结果。但我们无法确认这一点，因为实验记录只显示 SIM-001 和 SIM-002 各跑了一次。这恰好可以反过来说：只跑一次就成功了，说明可能存在运气成分。

---

### 7. 收敛问题

**问题严重程度：高（对协议设计有根本性威胁）**

**7.1 两个实验都未收敛**

SIM-001：6 轮未收敛，原因是催化行为偏差。
SIM-002：6 轮未收敛，原因归结为 token 截断。

但评估报告声称 SIM-002 "R1 有 9 个新发现，R2 有 3 个新发现"，展示了收敛趋势。这需要质疑：

(a) R3-R6 的收敛判断被截断了，我们不知道 R3 之后新发现数量是继续下降还是反弹。如果 R3 有 5 个新发现（反弹），整个收敛叙事就崩塌了。
(b) R1 到 R2 从 9 降到 3 可能只是因为 R1 是"初次扫描"（自然会发现最多），R2 是"增量扫描"——这是一个必然的下降，不是收敛的证据。

**7.2 催化可能永远不会收敛**

评估报告将未收敛归因于 token 截断——这是一个可修复的工程问题。但有一个更深层的可能性：催化从根本上不会在 6 轮内收敛。

证据：

- v0 催化每轮都产出新内容，没有任何收敛迹象。这被归因为"催化行为偏差（战略顾问模式）"。但也可以解释为：有 3 个 Profile、1 个触发事件、6 轮互动，系统中的信息量远大于催化在 6 轮内能穷尽的量。
- v1 催化的翻译稳定在 4 条/轮，6 轮全部是 4 条，没有下降趋势。如果每轮都能找到 4 条"新"翻译，什么时候会找到 0 条？
- LLM 有一种自然倾向：总是能从已有素材中找到新的模式。这不是因为新模式真的存在，而是因为 LLM 擅长生成合理化的关联。你给 LLM 任何两段文本，它都能找到一个"可能的等价关系"。

如果催化的收敛判断实质上是"我决定不再寻找新关联了"而不是"我真的找不到新关联了"，那么收敛机制就不是在检测信息差的消除，而是在检测 LLM 的"疲劳"——这是一个完全不同的概念。

**7.3 收敛机制的工程可靠性**

即使修复了 token 截断，收敛检测仍然完全依赖催化 Agent 的自我报告（字符串匹配 "本轮没有新发现"）。v0 的教训已经证明 LLM 的自我报告不可靠。v1 的格式约束改善了形式遵从，但实质可靠性未经验证——催化有可能在 R5 强制写"新发现数量：0"只是因为格式要求它做判断，而不是因为它真的找不到新东西了。

---

## 偏差风险矩阵

| 偏差类型 | 风险等级 | 证据 | 建议 |
|---------|---------|------|------|
| **确认偏差** | 极高 | GT 设计者 = prompt 设计者 = 评估者；实验只能证实不能证伪；叙事框架偏向成功 | 引入独立评估者；增加消蚀实验；设计能证伪 v1 优越性的对立实验 |
| **观测偏差** | 高 | 评估者知道每个输出来自 v0 还是 v1；评估者知道 v1 的设计意图；GT 评分标准弹性大 | 盲评；多评估者一致性检验；建立量规 |
| **结构性偏差** | 高 | N=1 场景、N=3 参与者、N=1 运行；合成 Profile 由实验者设计；触发事件为单一类型 | 多场景、多 Profile 组合、多次运行 |
| **幸存者偏差** | 中 | 无法确认是否存在未报告的失败运行；SIM-001 的"失败"正好为 v1 的"成功"提供对照 | 注册实验计划（pre-registration）；记录所有运行 |
| **测量偏差** | 高 | GT 评分无量规；约束违反计数无明确标准；"意外发现质量"无操作定义 | 建立评分量规；操作化所有指标 |
| **混杂变量** | 中 | v1 催化不越界 -> 端侧输出改变 -> 催化输入改变（路径依赖）；v1 输出格式与 GT 定义格式高度匹配 | 消蚀实验分离各变量贡献；考虑 GT 评分是否测量了"发现能力"还是"输出格式匹配度" |
| **随机性未控制** | 极高 | temperature=1.0（默认）；每个配置只跑 1 次；无均值/标准差 | 多次运行；报告统计量；至少一次 temperature=0 对照 |

---

## 下一步实验建议

### 最优先：修复实验可信度（在做任何新 prompt 迭代之前）

1. **多种子运行**：用当前的 v0 和 v1 配置，各跑 3 次（不设 random seed，让自然变化体现）。收集每次运行的 GT 评分、翻译数量、约束违反次数。报告均值和标准差。如果 v0 的 3 次运行 GT 评分是 [30%, 45%, 25%]，v1 是 [80%, 50%, 65%]，结论会和 [30%] vs [80%] 非常不同。成本：约 $12-18，约 2-3 小时。这是最低成本最高收益的改善。

2. **独立评估者盲评**：找至少一个不了解 v0/v1 差异的人，只给他催化输出（不标注来自哪个版本），让他独立评分 GT 发现率和约束违反。比较两个评估者的一致性。

3. **建立评分量规**：为 GT 发现率的每个分数（0-5）写一段操作定义。例如："5/5 = 催化明确使用两个参与者的原话，指出它们描述的是同一种能力/需求，并解释了等价关系"，"3/5 = 催化提到了两个参与者的相关话题，但没有明确指出它们之间的等价关系"。

### 次优先：扩展实验覆盖面

4. **第二个场景**：设计一个完全不同的触发事件和参与者组合（最好由不同的人设计 GT）。如果 v1 在新场景上也优于 v0，结论的可信度大幅提升。如果不优于，说明 v1 的优势可能是特定于当前场景的。

5. **消蚀实验**：分别测试 v1 的三个修改的独立贡献：
   - v1a：只改翻译优先级，不改约束和格式
   - v1b：只改禁止性约束，不改翻译和格式
   - v1c：只改强制格式，不改翻译和约束
   - 各跑 1-3 次，与完整 v1 和 v0 对比

6. **收敛验证**：修复 token 截断后（max_tokens 提升到 4500），跑 3 次 v1。观察：(a) 是否真的在某轮收敛？(b) 如果收敛，是因为真的没有新信息还是因为格式约束使然？(c) 收敛轮次在 3 次运行间是否一致？

### 远期：方法论基础设施

7. **实验预注册**：在运行任何新模拟之前，写下假说、预期结果、评估标准。事后不允许修改。这可以从根本上遏制确认偏差。

8. **自动化评估管线**：开发程序化的评估工具，至少覆盖约束违反检测（关键词匹配 + LLM 分类）和收敛趋势检测（语义相似度）。减少对人工主观判断的依赖。

9. **真实 Profile 替换**：合成 Profile 的同质结构是端侧同质化的重要原因，也使得 GT 的外部有效性存疑。社区 Profile 收集后应该作为最高优先级重跑实验。

---

## 审查者备注

以上审查可能给人一种印象：这个实验毫无价值。这不是审查的结论。

EXP-009 作为一个**探索性 prompt 迭代工具**是有效的。它成功识别了 v0 催化 prompt 的三个明确问题（翻译缺失、约束越界、收敛自检缺失），并通过定向修复产出了 v1。v1 在这三个维度上的改善是可以从输出文本中直接观察到的定性事实，不需要统计检验。翻译从 0 变成每轮都有，约束违反从频繁变成几乎没有——这些是真实的改善，即使只跑一次也能确认。

但将这种探索性观察包装为"30% -> 80%""压倒性成功"的定量结论，超越了证据所能支持的范围。数字给人一种精确感和确定性，而实际上这些数字背后是一个人对一次运行的主观评分。

在面向投资人、学术界或外部审查者时，诚实地呈现实验的局限性，比夸大结论更有说服力。一个承认自己 N=1 但有清晰发现的实验报告，比一个声称"压倒性成功"但经不起方法论检验的报告更值得信赖。
