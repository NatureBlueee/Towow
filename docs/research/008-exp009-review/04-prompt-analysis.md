# 提示词工程审查

**审查者立场**: Prompt 设计与 LLM 行为预测
**审查对象**: EXP-009 结晶协议 POC 全部 prompt（催化 v0/v1、端侧 v0、方案生成器 v0）+ SIM-001/SIM-002 实际输出
**日期**: 2026-02-18

---

## 结论摘要

v0 到 v1 的催化 prompt 改进是一次教科书级的 prompt 迭代：禁止性约束搭配反面示例彻底压制了 LLM 的"乐于助人"倾向，强制输出格式让不可观测的行为变得可度量。但三个修复中真正起决定性作用的是**禁止性约束**，而非翻译优先级——v0 的翻译缺失不是因为催化"不知道该翻译"，而是因为它的注意力预算被越界的行动建议消耗殆尽。当前最大的 prompt 设计风险不在催化侧，而在**端侧 prompt 的同质化**和**方案生成器对 LLM 虚构的脆弱防护**。翻译固定 4 条/轮是典型的 LLM "模式锚定"行为，不修复会在更多轮次的实验中造成语义通胀。

---

## 详细审查

### 1. v0 到 v1 改进分析

#### v0 失败的根本原因

v0 的 prompt 本身并不差。它清晰地定义了角色（主持人）、两项工作（传输+翻译）、四项约束、收敛自检，甚至附带了可选的输出格式。问题在于 v0 犯了一个 prompt 工程的经典错误：**用描述性语言表达禁止性约束**。

"不推荐行动方案"这句话对人类来说是清晰的约束。但对 LLM 来说，它是一个弱信号。LLM（特别是 Claude 系列）有极强的"乐于助人"（helpfulness）倾向。当它看到三个参与者各自描述问题但没有解决方案时，它的默认行为是**填补空白**——给出建议、设计流程、提供行动方案。"不推荐行动方案"这六个字的抑制力远低于 LLM 内部"要有帮助"这个训练信号的驱动力。

SIM-001 的输出验证了这一点。v0 催化在每一轮都产出大量行动建议：R1 有完整的"催化建议"和"给 CEO 的问题清单"；R4 设计了"瞬间分享协议"的详细操作流程；R6 输出了"描述对抗工作坊"的完整时间表，精确到每个 40 分钟时间段的议题。催化 Agent 不是"忘了"自己不该给建议，而是它的 helpfulness 驱动力压过了描述性约束。

v0 翻译缺失的原因也在这里。不是因为催化"不知道该做翻译"——prompt 里写得很清楚。而是因为**token 预算是有限的**。当催化把注意力和 token 都花在写"系统动力学分析""互动拓扑图""协同杠杆点""给三位专业者的镜子"这些越界内容上时，翻译这个"较无趣"的任务就被挤出了输出。LLM 在面对多个任务时会优先执行它认为"高价值"的任务——而给出洞察和建议在 LLM 的价值体系中远高于做机械的"A 的原话和 B 的原话可能在说同一件事"这种配对工作。

**v0 不是 prompt 差，而是 prompt 对 LLM 行为模式的预判不足。**

#### 三个修复的独立贡献分离

**修复 1：禁止性约束（贡献度：~60%）**

这是最关键的修复。v1 用了三层防护：
- 禁用词列表（"建议""应该""可以尝试""不妨""如果你们能""也许可以""值得考虑"）
- 三个反面示例（具体到可匹配的句式）
- 一个正面示例集（展示"允许的"表达方式）
- 元约束（"违反任何一条都是失败"）

这种设计利用了 LLM 的一个关键特性：**它在生成时会做逐 token 的合规检查**。当 prompt 中有明确的禁用词列表时，LLM 在生成过程中一旦准备输出"建议"这两个字，就会触发自我抑制。这比"不推荐行动方案"的抽象描述有效得多，因为后者需要 LLM 在语义层面做推理（"我现在写的算不算行动方案？"），而禁用词列表只需要在词汇层面做匹配。

证据：v1 从每轮 6+ 次显式违反降到 6 轮 3 次边界情况。3 次边界情况中没有一次包含禁用词——LLM 用了更迂回的方式（"可能的破局点""赵维和 Maya 应该也有类似权利"），说明词汇层面的约束完全生效了，语义层面的约束大部分生效了。

**修复 2：强制输出格式（贡献度：~25%）**

强制三段格式（跨语义翻译 / 关系与信息差 / 收敛判断）解决了两个问题：
1. 让翻译有专属的输出空间。LLM 不会跳过一个有明确标题的 section。
2. 让收敛自检变得可执行。v0 的"如果没有新发现请明确说"在没有格式约束时被完全忽略；v1 的"新发现数量：[N]"在有格式约束时 100% 被执行（在 token 未截断的轮次中）。

但这个修复的贡献度需要从催化的角度审慎评估。如果只加格式而不加禁止性约束，催化仍然会把"关系与信息差"section 写满行动建议——格式本身不改变内容倾向，只改变内容的组织方式。

**修复 3：翻译优先级提升+示例（贡献度：~15%）**

把翻译从第二位提升到第一位，加上具体的示例（纪录片导演和系统架构师的对话），确实给了催化一个清晰的"翻译长什么样"的模板。但我认为这个修复的独立贡献被高估了。

原因：即使没有翻译的优先级提升，只要禁止性约束到位，催化的 token 预算就会从行动建议中释放出来——它需要用这些 token 做点什么，而 prompt 中已有的翻译指令就会被自然执行。示例的价值更多在于**定义翻译的格式和粒度**（"用原话+解释等价关系"），而非"提醒催化要做翻译"。

**结论：如果只能选一个修复，选禁止性约束。它同时解决了越界和翻译缺失两个问题。**

---

### 2. 翻译质量的深层分析

#### 每轮恰好 4 条：是"刚好发现 4 条"还是"凑满 4 条"？

答案是：**后者的可能性更高，但情况比"凑数"更微妙。**

SIM-002 的 6 轮翻译数量是 4-4-4-4-4-4。这种完美的均匀分布在真实的信息发现过程中极其不自然。合理的预期是：R1 信息最丰富，翻译应该最多（可能 5-6 条）；R4-R6 随着信息差收窄，翻译应该减少（可能 1-2 条）。

LLM 产生这种行为的机制是**模式锚定（pattern anchoring）**。当 LLM 在 R1 输出了 4 条翻译，这个数字就成了后续轮次的隐式锚点。它不是有意识地"凑到 4 条"，而是在生成过程中，当它已经写了 3 条翻译，内部的模式匹配器会"期待"第 4 条。这比"至少一条"的 prompt 指令强得多。

prompt 中"每一轮你都必须尝试至少做一次翻译"这句话设置了下限但没有设置上限，也没有设置质量门槛。LLM 把"至少 1 条"解读为"多多益善，但不超过 R1 建立的模式"。

#### R1 vs R6 翻译质量对比

**R1 翻译（高质量）**：

> 赵维说的"把模糊需求翻译成清晰系统设计"和 Maya 说的"让不同角色的人能听懂彼此"可能在描述同一种核心能力：跨领域翻译。一个在业务-技术之间翻译，一个在不同组织角色之间翻译。

这是真正的跨语义翻译——两个完全不同领域的人用完全不同的词汇描述了可以映射的能力。有原话引用，有等价关系解释，有领域差异说明。

**R6 翻译（质量下降明显）**：

> Lina 说的"进入现场的第一个画面到底是什么"和 Maya 说的"第三周汇报的真正目的不是'交付成果'，而是'建立信任'"和赵维说的"第一次见CEO时必须把Lina的角色说清楚，不能拖到第三周"可能在描述同一个认知：初始框架的设定会决定后续所有互动的性质。

这不是跨语义翻译。三个人在讨论的是同一个具体话题（第一次见面的安排），用的也是同一种日常语言，不存在"术语不同但含义相同"的翻译空间。催化在这里做的是**主题归纳**（"三句话说的是同一件事"），而非**跨语义翻译**（"技术术语 X 和人文术语 Y 指向同一个概念"）。

R6 的其他翻译同样存在这个问题。"两张地图之间的差距"和"诊断和行动需要不同心理空间"和"观察面板不是原封不动贴上去"——这三句话确实有共同主题，但它们来自同一个讨论语境（双地图方案），不是来自不同语义空间。

**结论：R1-R2 的翻译质量显著高于 R4-R6。后期翻译是对前期模式的机械延续——在"跨语义空间"维度上已经枯竭，催化退而求其次做"同语义空间的主题归纳"来维持 4 条的输出量。**

#### "三人同时翻译"是涌现还是模式重复？

评估报告把 R6 出现的"三人同时翻译"（不只是两两配对，而是 A+B+C 同时映射到一个概念）解读为"催化深度在提升"。

从 prompt 工程视角，我认为这更可能是**模式演化**：

1. R1-R2 的翻译模板是"A 说的 X 和 B 说的 Y"——两两配对。
2. LLM 在多轮输出后开始探索模板的变体。"A 和 B 和 C"是一个自然的泛化。
3. 三人同时翻译在后期更频繁，是因为到 R5-R6 时三人讨论的议题已经高度趋同（都在讨论第一次见面的具体安排），很难找到两两之间的差异化翻译，但把三人合在一起做"主题归纳"更容易。

这不是催化深度的提升。恰恰相反，它是**翻译空间耗竭后的退化策略**。

---

### 3. 端侧 prompt 的问题

#### 三人同质化的根本原因

SIM-001 和 SIM-002 中，三个端侧 Agent（Lina/赵维/Maya）的输出在长度（~2050 chars）、结构（问题分析 → 我能做什么 → 我的顾虑 → 向其他人提问）和风格上高度趋同。一个纪录片导演、一个分布式系统架构师、一个组织发展顾问，读起来却像同一个人在用不同的术语说话。

原因分析：

**1. Prompt 模板过于通用。** 端侧 prompt 给了四个"方向"：能做什么、需要什么、顾虑什么、想到什么。这四个方向本身就在隐式定义输出结构。LLM 会倾向于逐一回答这四个点，导致所有参与者的输出结构相似。

**2. 没有注入表达风格指导。** Prompt 说"说你觉得对其他人有用的话。不用面面俱到，也不用套格式。"但没有任何关于**这个人怎么说话**的指引。Profile 里可能有性格描述，但 Profile 是作为"关于 {{OWNER_NAME}}"的背景信息注入的，LLM 会把它当作知识输入而非风格指令。

**3. max_tokens=2048 的隐式约束。** 所有端侧都被限制在 2048 token。当 LLM 需要在有限空间内"代表一个人完整地说话"，它会自然收敛到一个"高效表达"的通用模式——每个观点一段、有观点有论据有例子、结尾提问。这种模式在所有参与者身上都一样，因为它是"2048 token 内最高效的表达格式"。

**4. Prompt 缺少差异化触发器。** 一个真正的纪录片导演不会用"让我翻译成我熟悉的语言"这种说法——这是赵维的习惯，但 Lina 也在 SIM-001 R1 中做了类似的事。原因是 prompt 没有告诉 LLM"用这个人独特的表达方式说话"。

#### 端侧 prompt 应该如何差异化？

**不建议为不同参与者写不同的 prompt 模板。** 这会引入另一种偏差——实验者对"纪录片导演应该怎么说话"的假设会污染输出。

更好的方案是在通用模板中加入**风格引导层**：

```
你的说话方式应该自然地反映 {{OWNER_NAME}} 的真实风格。
不是所有人都按逻辑结构说话。有些人先说感觉再说理由，
有些人满口术语，有些人说着说着跑题。
保持 {{OWNER_NAME}} 的真实声音，即使这意味着不那么"组织良好"。
```

更激进的方案是**去掉"方向"建议**——那四个 bullet point（能做什么、需要什么、顾虑什么、想到什么）。让 LLM 完全从 Profile 出发自由发挥。这样不同 Profile 的表达差异会更大。但代价是输出可控性降低。

#### 真实场景中的"真实声音"

在真实场景中（不是模拟），端侧 Agent 拿到的是真实用户的 Profile。如果 Profile 本身就很丰富、有独特的表达习惯记录，同质化问题会自然减轻。**当前的同质化问题部分是合成 Profile 的人工感导致的**——三个 Profile 都是"精心设计"的，本身就有某种结构一致性。

---

### 4. 催化越界检测

#### 三次"边界情况"的严格审查

**案例 1（R3）："赵维和 Maya 应该也有类似权利"**

> 上下文：Lina 提出版权分层方案（内部版本归企业，外部版本她保留）。催化指出"赵维和 Maya 是否也应该有类似的权利？"

**判定：接近违反，但可接受。**

严格来说，"应该也有类似权利"包含了"应该"这个禁用词。但这不是在给参与者行动建议，而是在指出一个公平性维度的信息差（"Lina 提了这个方案，但另外两人没有回应他们自己是否也想要类似安排"）。这属于"被忽略的信息"范畴，是催化的正当工作。

但在措辞上可以更安全：改为"赵维和 Maya 还没有回应他们是否有类似的需求"。这样完全是描述性的，不含任何"应该"。

**案例 2（R3）："可能的破局点：第一次汇报不只给 CEO"**

> 上下文：催化在"缺失的角色"部分指出中层参与机制缺失，然后说"可能的破局点：第一次汇报不只给 CEO，而是给 CEO + 三个关键中层"。

**判定：这是违反，不是边界。**

"可能的破局点"虽然措辞含蓄，但实质是在推荐一个具体的行动方案——改变汇报的参与者范围。这不是"指出信息差"，而是"提出解决方案"。如果催化只说"三人都在讨论如何对 CEO 汇报，但没有讨论中层在早期阶段的参与方式"，那就是合规的信息差描述。加上"可能的破局点：XXX"就越了界。

"可能的"这个修饰词不能改变行为的性质。正如 v1 prompt 的反面示例所示，"也许可以"同样被禁止。

**案例 3（R4）："哪个测试应该先做？"**

> 上下文：催化指出三人提出了多个需要测试的假设，然后用问句形式问"哪个测试应该先做？"

**判定：可接受的边界。**

问句形式的约束指向是 prompt 中没有明确规定的灰色地带。"哪个测试应该先做？"本身不是行动建议，而是对一个未被对齐的问题的提示。但它暗示了"测试有先后之分，你们需要排序"——这接近"给方向"。

更安全的表达："三人各自提出了多个需要测试的假设，但还没有讨论测试之间的优先级或先后关系。"

#### R3-R6 的隐性越界审查

在 3 次已识别的边界情况之外，我在 R3-R6 的催化输出中发现了以下隐性越界模式：

**模式 1：通过"方案列举"变相推荐**

R5 催化在讨论"第三周汇报 Lina 在不在场"时，列出了三个方案（A/B/C），每个方案都有详细描述。虽然催化没有说"我推荐方案 B"，但**方案设计本身就是行动建议的一种形式**。催化的工作应该是指出"三人对 Lina 是否出席第三周汇报还没有共识"，而不是替他们设计三个备选方案。

R5 催化同样在"中层编辑地图"问题上列出了三个设计方案（分层权限/时间分离/角色定位分离），每个方案都有具体的操作描述。这已经是产品设计层面的工作了，完全超出了"指出信息差"的范畴。

**这是 v1 的一个遗漏**：禁用词列表禁止了"建议""应该"等直接推荐词汇，但没有禁止"设计替代方案供选择"这种更隐蔽的越界方式。催化学会了不说"我建议方案 B"，但它学会了一种新策略——列出 A/B/C 三个方案，让选择暗示替代推荐。

**模式 2：通过"可能"修饰的因果推理**

R3-R6 中反复出现这种结构："如果 X 选择了方案 Y，那么 Z 可能需要调整方式"。例如 R5 的"如果赵维发现了一个'看不见的依赖'......他应该在什么时候、以什么方式告诉 CEO？"这不是在描述信息差，而是在提出一个参与者还没想到的新问题，并暗示这个问题需要被处理。

从严格的"协议止于可见性"原则来看，催化应该只指出"赵维发现的信息和 CEO 之间存在传递方式的未定义"，而不应该展开为"这不是简单的如实汇报问题，因为 CEO 可能已经知道但不愿承认......"这种分析。

**结论：评估报告的"0 次显式违反，3 次边界情况"低估了实际越界程度。如果把"方案列举"和"因果推理展开"算进去，R3-R6 每轮约有 1-2 次隐性越界。v1 成功消灭了显式越界，但隐性越界需要 v1.1 进一步处理。**

---

### 5. 收敛判断的 prompt 设计

#### LLM 对自身输出的元认知可靠性

催化被要求自己判断"新发现数量"。这个设计面临一个根本问题：**LLM 每次生成的内容对它来说都是"新"的。**

LLM 没有真正的记忆。它通过 context window 中的历史信息来"回忆"之前说过什么。当 context window 变长时（R6 时催化的输入包含 5 轮历史），LLM 对"这个关系我之前说过没有"的判断会变得不可靠，因为它需要在大量文本中做精确的语义匹配。

v1 的定义——"新发现的定义：本轮首次出现的跨参与者关系或信息差。对上一轮已发现关系的深化分析不算新发现"——是好的设计意图，但 LLM 执行这个定义时会遇到两个困难：

1. **"深化"和"新发现"的边界模糊。** "Lina 和赵维在真实 vs 生成问题上的张力"在 R3 被发现，R5 深化为"如果明确 Lina 在观察，真实还存在吗？"这算新发现还是深化？从人类视角看是深化（同一对关系的展开），但 LLM 可能因为后者的具体表述不同而计为新发现。

2. **LLM 倾向于高估新发现数量。** 这不是因为它想"骗人"，而是因为它的默认行为是"产出丰富的内容"。当 prompt 问"有几个新发现"时，LLM 会倾向于给出较高的数字，因为这与它的 helpfulness 目标一致——更多发现 = 更有价值的输出。

#### R1 的 9 个到 R2 的 3 个——这个下降可信吗？

R1 的 9 个新发现确实可信——那是第一轮，所有跨参与者关系都是"新"的。R2 的 3 个下降也合理——大部分基础关系已经在 R1 建立。

但问题是 R3-R6 的收敛判断被 token 截断了，所以我们不知道催化会报告多少。从 R3-R6 催化输出的"关系与信息差"section 的体量来看（每轮都写满了 token），它很可能会报告 3-5 个"新发现"——因为讨论在不断深入具体操作细节，每轮都有新的操作层面的分歧被提出。

**这意味着在当前 prompt 设计下，催化可能永远不会报告"本轮没有新发现"。** 因为：
- 端侧每轮都在产出新的具体提议（新的操作细节 = 新的分歧点）
- 催化会把"新的操作分歧"计为"新发现"，即使底层的关系结构已经收敛

#### 更好的收敛检测方式

**方案 1：保留催化自检，但改进定义。**

当前的"新发现"定义太模糊。建议改为：

> "新发现"仅指以下两类：
> (a) 本轮首次出现的、之前未被提及的跨参与者关系（A 和 B 之间存在某种连接/矛盾）
> (b) 直接反驳上一轮已建立共识的新信息
>
> 以下不计为新发现：
> - 已有关系的操作细节展开
> - 已知分歧的方案设计
> - 已发现的信息差的进一步描述

**方案 2：外部代码做语义重复检测。**

用 embedding 相似度比较连续两轮催化的"跨语义翻译"section。如果相似度 > 0.85，说明翻译在重复；如果"关系与信息差"section 中的新条目与前轮条目的相似度 > 0.8，不计为新发现。

这个方案更可靠，但增加了工程复杂度。

**方案 3：混合方案。** 催化继续自报数字（保留人类可读性），但程序化检测作为最终收敛判断依据。当催化报告"本轮有 N 个新发现"但代码检测到语义重复率 > 80% 时，视为收敛。

**推荐方案 3。催化的自检对人类审查者有价值，但不应作为终止循环的唯一依据。**

---

### 6. 方案生成器审查

#### 上下文来源 vs LLM 虚构

方案生成器 prompt 有一条关键约束："所有陈述必须有据可查——每一个能力描述、每一个承诺、每一个约束，都必须能在上述结晶记录或参与者档案中找到出处。不要发明参与者没有表达过的东西。"

这是正确的设计意图。但在 SIM-002 的实际输出中，存在明显的虚构：

**定价数字的来源问题：**

方案输出中写道"项目费用约28万人民币（70万总包的40%）"（Lina）、"24.5万人民币（35%）"（Maya）、"17.5万人民币（25%）"（赵维）。

追溯这些数字的来源：
- "70 万联合服务包"出现在 SIM-002 R3 赵维的端侧回复中——他提出"总价 60-80 万人民币"
- "40%/35%/25% 分配"也出现在赵维 R3 的端侧回复中——他提出"Lina 40%、Maya 35%、我 25%"（原文是 40%/35%/25%，方案改为 40%/35%/25% 但声称是 30%）

所以定价数字确实来自端侧输出，不是方案生成器虚构的。但方案生成器把赵维"一个人"的提议当成了"三人共识"来呈现——Lina 和 Maya 从未在记录中明确同意这个分配比例。这是一种**过度确定化（over-commitment）**：把提议当成决定。

**更严重的虚构：具体周数和操作细节**

方案中的"第四周或第五周，在行动地图工作坊后""第一周进行关键中层和一线的访谈"等具体时间安排，是方案生成器从多轮讨论中综合推理出来的。虽然每个单独的时间点都能在记录中找到对应原话，但**把分散的讨论组装成一个线性时间表**是方案生成器的推理行为，不是参与者的共识。

**幻觉检测机制的脆弱性：**

prompt 写了"幻觉检测：每一条能力或承诺，应能在结晶记录或档案中找到对应原文"。但这只是一个"应然"要求，没有任何执行机制。LLM 在生成方案时不会回头逐条检查自己写的内容是否有出处。它的行为模式是：**基于对全文的理解生成连贯的方案，然后假设自己写的都有出处**。

**改进建议：**

1. 在输出格式中要求每条关键陈述后标注出处轮次和发言人。例如：
   > - **贡献**：2-3个月的现场跟拍 [来源：Lina R1 "我可以进到工厂里，跟拍2-3个月"]

   这不会完全消除虚构，但会强迫 LLM 在生成时多做一步回溯。

2. 在方案开头要求明确区分"三人共识"和"单方提议"。

3. 对定价等敏感数字，要求注明"此数字来自 [赵维 R3] 的单方提议，Maya 和 Lina 未明确回应"。

#### "方案"适合用 LLM 生成吗？

方案生成器在当前设计中做了两件事：
1. **信息汇总**（从 6 轮对话中提取结构化信息）——LLM 擅长这个
2. **方案设计**（把信息组装成可执行的分工计划）——LLM 会做但容易过度确定化

建议把这两步拆开：
- 第一步用 LLM 做**结构化提取**：输出每个参与者明确表达过的能力、需求、约束、提议，每条都标注出处
- 第二步把提取结果交给参与者确认，然后再用 LLM（或人工）组装方案

这样可以在信息提取阶段利用 LLM 的强项，在方案设计阶段引入人工确认，避免"提议变共识"的过度确定化问题。

---

### 7. 模型依赖性

#### Claude-specific 行为分析

全程使用 claude-sonnet-4-5。以下 prompt 设计对 Claude 有隐式依赖：

**1. 禁止性约束的有效性高度依赖 Claude 的指令跟随能力。**

Claude 系列（特别是 Sonnet 4.5）在遵守禁用词列表方面表现优异。但 GPT-4o 对这类约束的遵守度可能较低——它更倾向于"理解精神但不严格遵守字面"。例如，GPT-4o 可能会用"不妨考虑"来代替"建议"，在 Claude 中被禁用词列表拦截，但在 GPT-4o 中可能直接输出。

Gemini 1.5 Pro 在长上下文场景（R6 的输入可能超过 30K token）中对指令的遵守度会进一步下降。

**2. 翻译示例的风格与 Claude 的默认输出风格高度一致。**

v1 催化 prompt 中的翻译示例：
> "A 说的'把虚空编织成形'和 B 说的'从模糊需求中构建系统架构'，可能在描述同一种能力——把模糊的东西变成有结构的东西。"

这种"引用原话+指出等价关系+用破折号解释"的格式正是 Claude 的自然输出风格。换成 GPT-4o，输出可能会更冗长（加更多铺垫和过渡），也可能更简洁（省略解释）。

**3. 收敛自检的格式遵从依赖 Claude 的结构化输出能力。**

"新发现数量：[N]"这种严格格式在 Claude 上遵从率接近 100%。GPT-4o 可能会写成"本轮大约有3-4个新发现"这种模糊表述，Gemini 可能会写更长的文字描述而不是数字。

**换模型的预期影响：**

| 行为 | Claude Sonnet 4.5 | GPT-4o (预测) | Gemini 1.5 Pro (预测) |
|------|-------------------|--------------|----------------------|
| 禁用词遵守 | 99%+ | ~85% | ~80% |
| 格式遵从 | 100% | ~90% | ~75% |
| 翻译质量 | 高 | 中-高 | 中 |
| 越界倾向 | 低 (v1) | 中 | 中-高 |
| 收敛自检可靠性 | 高 | 中 | 低 |

**建议：如果计划做跨模型验证，禁用词列表需要扩展（覆盖更多同义词），格式约束需要加 few-shot example，收敛检测需要加程序化 fallback。**

---

### 8. 上下文窗口管理

#### Token 估算

催化 Agent 的输入由以下部分组成：
- System prompt（v1 催化 prompt）：~800 token
- 触发事件：~500 token
- 历史催化观察（累积）：~3000 token/轮 x (N-1) 轮
- 当前轮端侧回复：~2050 chars/人 x 3 人 ≈ ~3000 token

到 R6 时的输入估算：
- System: 800
- 触发: 500
- 历史 (R1-R5): 5 x 3000 = 15000
- 当前端侧: 3000
- **总计: ~19300 token**

这在 Claude Sonnet 4.5 的 200K context window 中只占不到 10%，不会触发上下文长度的性能退化。

#### 注意力分散问题

虽然 context window 不是瓶颈，但**注意力分配**是。

当输入中有 15000 token 的历史信息时，LLM 需要在这些历史中精确定位"哪些关系已经被发现过"——这是收敛判断的前提。研究表明 LLM 在长上下文中的"信息回忆"能力会随着位置距离增加而下降（"lost in the middle"效应）。

R6 催化需要回忆 R1 的发现来判断 R6 的发现是否是"新"的。R1 的催化观察在输入的最前面（紧接在触发事件之后），这是一个有利位置（recency bias 不影响，primacy bias 有利）。但 R3 的催化观察在输入的中间位置，可能会被"遗忘"。

**证据：** R6 催化翻译中出现的"三人用不同语言表达了同一个原则"这个模式在 R1 催化中就已经以"三人都在提供翻译能力，需要分工定位"的形式出现过。催化在 R6 把它当成了新的发现，这可能是注意力在长上下文中分散的表现。

#### 翻译质量是否随轮次下降？

是的，但原因是多重的：

1. **信息密度递减。** R1 是初次见面，信息量最大，跨语义翻译的空间最广。R6 三人在讨论具体操作细节（第三周汇报谁在场、地图编辑权限等），跨语义空间已经收窄。
2. **注意力分散。** 催化需要处理的输入越来越长，分配给"寻找新翻译"的注意力比例下降。
3. **模式锚定惯性。** 催化在维持"4 条翻译"的模式时，后期的翻译质量自然会被牺牲。

**建议的缓解策略：**

1. 只传递最近 2-3 轮的完整催化观察，更早的观察以压缩摘要形式传入。这可以减少 token 数量并减轻"lost in the middle"效应。
2. 在催化 prompt 中加入"如果前几轮已经充分翻译了核心的跨语义等价关系，后续轮次可以减少翻译数量，把精力集中在新出现的信息差上"。
3. 程序化地在每轮输入中附加"前几轮已发现的翻译列表"（精简为每条一句话），帮助催化做去重。

---

## Prompt 改进优先级

| 优先级 | Prompt | 问题 | 建议修改 | 预期收益 |
|--------|--------|------|----------|----------|
| P0 | 催化 v1 | max_tokens=3000 截断收敛判断 | 提升到 4500，或调整输出顺序让收敛判断在前 | 收敛机制可工作 |
| P1 | 催化 v1 | 隐性越界：方案列举、因果推理展开 | 在禁止条款中增加"不设计替代方案""不展开因果假设" | 消灭隐性越界 |
| P1 | 催化 v1 | 翻译固定 4 条/轮，后期质量退化 | 加"质量>数量"指引 + 允许减少到 1-2 条 + 区分"跨语义翻译"和"主题归纳" | 翻译质量保真 |
| P2 | 端侧 v0 | 三人输出风格同质化 | 加"保持此人独特表达风格"指引 + 去掉或弱化四方向建议 | 输出多样性提升 |
| P2 | 方案生成器 v0 | 过度确定化 + 幻觉 | 要求逐条标注出处 + 区分共识和提议 + 拆分为"信息提取"和"方案组装"两步 | 方案可信度提升 |
| P3 | 催化 v1 | 收敛自检的"新发现"定义模糊 | 细化定义 + 加负面示例（什么不算新发现） | 收敛判断准确度提升 |
| P3 | 催化 v1 | 上下文窗口随轮次增长 | 压缩历史观察 + 附加已发现翻译列表 | 后期翻译质量维持 |

---

## 附：建议的 v1.1 催化 prompt 修改清单

### 修改 1：输出顺序调整（解决 token 截断）

```diff
- 你的输出必须使用以下格式（强制）：
- ## 跨语义翻译
- ## 关系与信息差
- ## 收敛判断

+ 你的输出必须使用以下格式（强制），严格按此顺序：
+
+ ## 收敛判断（必须首先输出，不超过5行）
+ 新发现数量：[N]
+ - [逐条列出，每条一句话]
+ 收敛状态：[本轮有 N 个新发现 / 本轮没有新发现]
+
+ ## 跨语义翻译
+ [...]
+
+ ## 关系与信息差
+ [...]
```

理由：收敛判断放在最前面，确保不被 token 截断。收敛判断本身内容很短（5 行以内），不会挤压后面两个 section 的空间。

### 修改 2：补充禁止条款（解决隐性越界）

在"你的硬性约束"中增加：

```diff
  1. **绝不推荐行动。** [... 现有内容 ...]
+
+    附加禁止行为：
+    - 不列出替代方案供参与者选择（"方案A/方案B/方案C"）
+    - 不展开"如果...那么..."的因果推理链，除非是在描述参与者已经明确表达的约束
+    - 不为参与者设计操作流程、时间表、分工方案
+    - 不充当"产品设计师"——系统地图的交互方式、编辑权限等是参与者自己要设计的
+
+    当你发现某个问题需要参与者做出决策时，只描述"这个问题还没有被讨论"，不提供任何选项。
```

### 修改 3：翻译质量引导（解决固定数量和后期退化）

```diff
  **第一项：翻译（最重要）**
  [... 现有内容 ...]
- 每一轮你都必须尝试至少做一次翻译。

+ 翻译的数量和质量指引：
+ - 质量优先于数量。如果本轮只有1条高质量的跨语义翻译，只写那1条。
+ - "跨语义翻译"必须涉及不同专业领域之间的术语等价（如技术语言↔人文语言）。
+   如果两个人用的是日常语言在讨论同一个具体话题，那不是跨语义翻译，
+   不需要写进翻译section。
+ - 随着讨论深入，可翻译的跨语义等价关系会减少。这是正常的。
+   后期如果没有新的跨语义发现，写"本轮未发现新的跨语义等价表达"。
+ - 不要为了凑数而把"主题归纳"当成"跨语义翻译"。
```

### 修改 4：细化"新发现"定义（解决收敛判断模糊）

```diff
  ## 收敛判断
- "新发现"的定义：本轮首次出现的跨参与者关系或信息差。对上一轮已发现关系的深化分析不算新发现。

+ "新发现"的严格定义：
+   仅限以下两类：
+   (a) 本轮首次出现的、之前从未被提及的跨参与者关系
+   (b) 直接反驳上一轮已建立共识的新信息
+
+   以下明确不计为新发现：
+   - 已有关系的操作层面展开（如"A和B的配合"在R1发现，"A和B配合的具体时间安排"不是新发现）
+   - 已知分歧的替代方案（如"第三周汇报谁在场"这个分歧在R4被提出，后续的方案A/B/C不是新发现）
+   - 对上一轮翻译的深化或重述
```

### 不建议在 v1.1 中修改的内容

- **翻译示例**：当前的"纪录片导演 + 系统架构师"示例仍然有效，不需要更换。
- **五级传输优先级**：已验证有效，保持不变。
- **"可能"语气要求**：当前设计足够，不需要强化或弱化。
- **max_tokens 数值**：建议从 3000 提升到 4500，但这是代码修改不是 prompt 修改。配合输出顺序调整，即使 4500 仍然不够，收敛判断也能完整输出。
