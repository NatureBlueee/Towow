# H4 — 最小验证实验设计

> 创建日期：2026-02-07
> 状态：PRD 已细化
> 优先级：Tier 1（阻塞核心假设验证）
> 关联任务：A1（HDC 编码策略验证）、E3（端到端验证）、H2（Prompt 工程研究）

---

## 为什么做这件事

通爻网络的架构设计已经确立了一系列核心机制：HDC 超向量编码、多 Agent 协商、响应范式、投影即函数。这些决策不是"要不要做"的问题——它们是通爻的存在论承诺（Agent 在网络中的存在形式就是超向量，协商是价值发现的基本方式）。但"效果有多好"目前全靠推理和信念，没有量化数据支撑。

不验证，我们不知道 HDC 编码后语义保留了多少、协商比单次调用好多少、响应范式是否真的能发现意外价值。这意味着无法迭代优化（不知道往哪个方向调），无法对外解释（"我们相信它有效"说服力为零），也无法判断资源投入优先级（哪个环节是瓶颈？）。

这个任务的目标是：设计一套最小可执行的实验，用最低成本回答最关键的问题，把"信仰驱动开发"变成"数据驱动迭代"。

---

## 你要回答什么问题

### 核心问题

**通爻架构的 5 个核心假设，各自的实际效果如何量化？每个假设的优化方向在哪里？**

### 子问题清单

每个子问题可以独立回答，互不阻塞。

#### Q1：HDC 编码的语义保留度

> 将自然语言文本编码为 10,000 维二进制超向量后，语义相似性保持到什么程度？

具体衡量：
- 编码前（原始嵌入向量）的余弦相似度排序 vs 编码后（超向量）的汉明距离排序，Spearman 相关系数是多少？
- 在 100 对"语义相关 / 语义无关"的测试文本上，共振检测的准确率（precision / recall）是多少？
- 中文文本 vs 英文文本的编码质量差异有多大？

#### Q2：Bundle 操作的信息保留率

> 将 5-10 个子向量 bundle 在一起（模拟 Agent 画像 = 技能1 + 技能2 + 经历1 + ...）后，共振检测还准确吗？

具体衡量：
- 单一技能向量的共振准确率 vs bundle 后的共振准确率，下降了多少？
- Bundle 几个子向量后开始出现明显退化？（信息保留率曲线）
- 不同 bundle 策略（简单多数投票 vs 加权 bundle）对保留率的影响？

#### Q3：多 Agent 协商 vs 单次 LLM 调用的方案质量对比

> 多 Agent 协商产生的方案，比单次 LLM 调用（把所有信息一次喂给 LLM）产生的方案，好多少？好在哪？

具体衡量：
- 同一个需求，对比两种方式产生的方案，从以下维度人工评分（1-5 分）：
  - 方案完整性（是否覆盖了需求的各个方面）
  - 方案多样性（是否提供了不同角度的选项）
  - 意外价值（是否包含了需求者自己没想到的有价值内容）
  - 可操作性（方案是否具体到可以直接执行）
- context 多样性的贡献：多 Agent 各自贡献了什么不同的视角？这些视角是否在单次调用中被遗漏？
- 成本对比：多 Agent 协商消耗的 token 数 vs 单次调用的 token 数，质量提升能否 justify 成本增加？

#### Q4：响应范式发现"未知价值"的能力

> 在一个具体场景中，响应范式（广播 + 自主共振 + 协商）能否发现搜索范式（关键词匹配 / 标签过滤）发现不了的有价值组合？

具体衡量：
- 设计一个有"认知转变"潜力的场景（如：用户以为需要 X，实际上 Y 更好）
- 搜索范式方案 vs 响应范式方案的对比
- 用户事后评估："哪个方案让你发现了自己没想到的东西？"
- 被"意外发现"的参与者在搜索范式中是否会被过滤掉？

#### Q5：端侧共振检测的速度和能效

> 在 1000 个 Agent 规模下，HDC 共振检测的实际速度和资源消耗是多少？

具体衡量：
- 单次共振检测（汉明距离计算）的实际耗时（纳秒级）
- 1 条需求 vs 1000 个 Agent 画像的批量检测耗时
- 100 条需求/秒的持续负载下，CPU 占用率
- 与直接用嵌入向量做余弦相似度的速度对比（快多少倍？）
- 内存占用：1000 个 Agent 画像（各 10,000 维二进制）的实际内存

---

## 我们提供什么

### 设计原则（遇到抉择时的指南针）

以下原则与 H4 直接相关：

| # | 原则 | 与 H4 的关系 |
|---|------|-------------|
| 0.2 | 本质与实现分离 | 实验验证的是本质（机制是否有效），不是实现（具体参数） |
| 0.3 | 复杂度 O(N+M) | Q5 验证这个复杂度目标在实际中是否成立 |
| 0.4 | 计算分布在端侧 | Q5 验证端侧计算的可行性和效率 |
| 0.6 | 需求 ≠ 要求 | Q4 验证"不硬筛选"是否真的能发现更好的方案 |
| 0.7 | 复杂性从简单规则生长 | 如果核心机制有效，复杂性自然涌现 |
| 0.8 | 投影是基本操作 | Q1/Q2 验证投影（编码）的信息保留能力 |

### 设计文档（详细技术参考）

| 文档 | 相关 Section | 关注什么 |
|------|-------------|---------|
| `docs/ARCHITECTURE_DESIGN.md` | Section 0（设计原则） | 10 条原则的完整描述 |
| 同上 | Section 1（最小完整单元） | 核心流程、需求 formulation、自-我映射 |
| 同上 | Section 6.3（签名共振机制） | HDC 编码流程、三层过滤、共振检测、阈值策略 |
| 同上 | Section 6.5.6（ProfileDataSource） | 投影即函数的工程映射 |
| 同上 | Section 9（Skill 系统） | 6 个 Skill 的接口定义和 Prompt 草案 |
| `docs/DESIGN_LOG_003_PROJECTION_AS_FUNCTION.md` | 全文 | "投影即函数"洞察、极度简化原则 |
| `docs/CONTRIBUTION_TASK_CATALOG.md` | H4 条目、A1 条目 | 任务上下文和依赖关系 |

### 工具和 Skill

| 资源 | 用途 |
|------|------|
| **Arch Skill**（`.claude/skills/arch/SKILL.md`） | 讨论架构问题时的思维框架 |
| **Task Arch Skill**（`.claude/skills/task-arch/SKILL.md`） | 子任务拆分和评估方法 |
| **Dev Skill**（`.claude/skills/towow-dev/SKILL.md`） | 具体编码时的工程实践 |

### 已有代码和原型

| 资源 | 位置 | 用途 |
|------|------|------|
| Team Matcher 原型 | `website/` | 前端 + 后端可参考 |
| 演示场景数据 | `backend/demo_scenario.json` | "找技术合伙人"场景，7 个 Agent 的完整 Profile |
| 后端协商引擎 | `backend/team_composition_engine.py` | 现有协商逻辑可作为 baseline |

### 概念翻译提示（用实验者的语言解释通爻术语）

| 通爻术语 | 实验者理解 |
|---------|-----------|
| HDC 超向量 | 10,000 维二进制向量，类似 SimHash / Locality-Sensitive Hashing |
| 共振检测 | 汉明距离相似度计算，判断两个向量是否"足够相关" |
| 投影 | 将丰富的结构化数据（Profile）编码为低维表示（超向量）的过程 |
| Bundle | 多个超向量的逐位多数投票合并，类似信息的叠加 |
| 响应范式 | 广播需求 → 相关方自主判断并响应（vs 搜索范式：主动检索和筛选） |
| 协商 | 多个 LLM Agent 各自基于不同 Profile 生成 Offer，再由 Center Agent 综合成方案 |
| 需求 formulation | 将用户原始意图丰富化为更完整的需求表达（类似 query expansion） |
| k* 机制 | 期望响应数，用业务语言（"我想要 10 个响应"）替代技术参数（"阈值 0.65"） |

---

## 子任务分解

### H4.1 — 假设清单与实验优先级排序

**描述**：明确定义通爻的 Top 5 核心假设，将每个假设拆解为可测量的指标（metrics），并根据"关键度 x 可验证度"排出优先级。

**依赖**：无（可立即开始）

**交付物**：
- 假设清单文档（每个假设一句话 + 可测量指标 + 成功/失败判断标准）
- 优先级排序矩阵（关键度 x 可验证度 的二维图）
- 每个假设的"最小实验规模"估算（需要多少数据、多少时间）

### H4.2 — HDC 编码与 Bundle 实验（对应 Q1 + Q2）

**描述**：实现最小可运行的 HDC 编码 pipeline（文本 → 嵌入 → SimHash → 超向量），在中文测试数据集上验证语义保留度和 bundle 信息保留率。

**依赖**：H4.1（需要知道衡量指标）

**交付物**：
- 可运行的 Jupyter Notebook，包含：
  - 编码 pipeline 实现（sentence-transformers + SimHash）
  - 语义保留度测试（Spearman 相关系数）
  - 共振检测准确率测试（precision / recall / F1）
  - Bundle 信息保留率曲线（1 个到 10 个子向量）
  - 中英文对比数据
- 结论：编码质量是否满足共振检测需求？瓶颈在哪？

**注**：此子任务与 A1（HDC 编码策略验证）有重叠。如果 A1 已经启动，H4.2 可以直接使用 A1 的产出作为输入，聚焦于"验证结论"而非"探索方案"。

### H4.3 — 协商 vs 单调用对比实验（对应 Q3）

**描述**：在同一个需求场景下，分别用"多 Agent 协商"和"单次 LLM 调用"两种方式生成方案，人工评估质量差异。

**依赖**：H4.1（需要知道评估维度）

**交付物**：
- 实验设计文档：场景选择、Agent 配置、评估标准、评审员指南
- 至少 3 个不同类型的需求场景的对比结果（建议：1 个简单/标准需求 + 1 个复杂/模糊需求 + 1 个有"认知转变"潜力的需求）
- 每个场景的方案对比评分表（4 维度 x 1-5 分）
- Token 消耗统计和成本对比
- 结论：协商在什么类型的需求上优势最大？什么时候单调用就够了？

### H4.4 — 响应范式 vs 搜索范式场景验证（对应 Q4）

**描述**：设计一个有"意外发现"潜力的完整场景，分别用响应范式和搜索范式处理同一个需求，对比结果。

**依赖**：H4.1（需要知道成功标准）；可参考 H4.3 的部分基础设施

**交付物**：
- 场景设计文档：参与者 Profile（5-10 个）、需求设计（必须有"认知转变"潜力）、搜索方案的过滤规则
- 两种范式的方案对比
- "意外发现"分析：响应范式发现了什么搜索范式会过滤掉的？
- 用户评估（如果可能，找 3-5 个真实用户做盲测）
- 结论：响应范式在哪类场景有不可替代的价值？

### H4.5 — 端侧性能基准测试（对应 Q5）

**描述**：在实际硬件上运行 HDC 共振检测的 benchmark，验证架构文档中的性能预估。

**依赖**：H4.2（需要编码 pipeline 产出的向量数据）

**交付物**：
- Benchmark 脚本（可复用、可复现）
- 性能报告：
  - 单次汉明距离计算耗时（多次取中位数）
  - 批量检测（1 需求 x 1000 Agent）耗时
  - 持续负载（100 需求/秒 x 60 秒）的 CPU 和内存监控
  - 与嵌入向量余弦相似度的速度对比
  - 不同维度（5,000 / 10,000 / 20,000）的性能对比
- 结论：架构文档的 "<100ns 单次检测" 和 "<0.1ms 全网检测" 是否成立？

### H4.6 — 综合分析与迭代建议

**描述**：汇总 H4.2-H4.5 的实验结果，分析各假设的验证状态，提出优化方向和后续实验计划。

**依赖**：H4.2 + H4.3 + H4.4 + H4.5（全部完成后）

**交付物**：
- 综合分析报告（3000-5000 字）：
  - 5 个假设的验证结论（证实 / 部分证实 / 未证实 / 需更多数据）
  - 每个假设的关键发现和意外发现
  - 系统级瓶颈分析（哪个环节是当前最弱的？）
  - 优化方向建议（下一步应该投入在哪？）
  - V1 可以先用什么 workaround？V2 需要解决什么？
- 迭代实验计划（如果某些假设未完全验证，下一轮实验怎么做）

---

## 做完了是什么样

### 产出格式

- **假设清单与优先级**：Markdown 文档，1-2 页
- **实验 Notebook**：Jupyter Notebook（H4.2, H4.5），可直接运行复现
- **对比分析**：Markdown 文档（H4.3, H4.4），含数据表格和评分
- **综合报告**：Markdown 文档，3000-5000 字

### 产出规模

- 测试数据：至少 100 对中文文本用于编码测试（Q1/Q2），3 个场景用于协商测试（Q3），1 个完整场景用于范式对比（Q4）
- 评估维度：每个对比实验至少 4 个评估维度，每维度 1-5 分
- 性能数据：至少 1000 次重复测量取统计量

### 质量标准

**"做完了"的标准**（最低要求）：
- 每个假设有明确的验证结论（证实 / 未证实 / 不确定）
- 结论有数据支撑（不是"感觉有效"，而是"准确率 85%，比 baseline 高 15%"）
- 实验可复现（代码可运行、数据可获取、步骤可重复）

**"做得好"的标准**（加分项）：
- 发现了假设之外的 insight（"我们预期 X，但实验发现 Y，这意味着..."）
- 提出了具体的优化方向（"如果把编码维度从 10,000 提高到 20,000，预计准确率提升 Z%"）
- 实验设计本身可以复用（未来架构调整后，换个参数就能重新跑）

**"做得出色"的标准**（产生额外价值）：
- 实验结果改变了某个架构决策（如发现 bundle 信息损失太大，需要修改画像生成策略）
- 发现了"多 Agent 协商"产生意外价值的具体机制和条件——不仅证明"有效"，还解释了"为什么有效"
- 实验框架被 A1、H2 等其他任务直接复用
- 量化数据足以写入对外传播材料（投资人演示、技术博客）
- 对"失败"的假设有有价值的分析（"虽然 X 不如预期，但原因是 Y，解决方案可能是 Z"）

---

## 你必须遵守的

### 硬性约束

1. **HDC 是存在论承诺，不是可选技术方案**。实验的目的是量化效果和找到优化方向，不是质疑"要不要用 HDC"。如果编码效果不好，结论应该是"需要换编码策略"或"需要调参数"，而不是"不应该用 HDC"。

2. **多 Agent 协商的价值在于 context 多样性**。Q3 的实验设计必须保证每个 Agent 持有不同的 Profile 数据（不同视角、不同领域），而不是同质化的 Agent 做迭代优化。协商不是"多次优化同一个答案"，而是"多个不同视角碰撞出新东西"。

3. **响应范式 vs 搜索范式是根本不同的范式**。Q4 的对比不是"哪个更快"或"哪个更便宜"，而是"哪个能发现对方发现不了的价值"。特别关注搜索范式会过滤掉、但实际有价值的参与者。

4. **中文优先**。所有测试数据必须包含中文文本。通爻的第一个市场是中文市场。

5. **实验必须可复现**。所有代码、数据、评估标准必须记录，第三方可以独立复现结果。

6. **反脆弱**。每个实验必须有"如果假设不成立，我们能从结果中得到什么"的思考。失败的实验也是有价值的——它告诉我们什么不行，缩小了搜索空间。

### 对齐设计原则

- 设计原则 0.2（本质与实现分离）：实验区分"机制是否有效"（本质）和"参数是否最优"（实现）
- 设计原则 0.5（代码保障 > Prompt 保障）：Q3 的实验中，协商流程的控制逻辑（等待屏障、轮次限制）用代码实现，不靠 Prompt
- 设计原则 0.8（投影是基本操作）：Q1/Q2 本质上是在验证投影操作的信息保留能力

---

## 你可以自己决定的

### 方法选择

- 测试数据集的构造方式（自己写、从公开数据集采样、用 LLM 生成、混合使用）
- 嵌入模型的选择（sentence-transformers 的哪个具体模型、是否同时试多个）
- 人工评估的具体量表设计和评审流程
- 性能测试的硬件环境和测试工具
- Notebook 的组织结构和可视化方式

### 范围调整

- 如果某个子问题在实验过程中发现不重要（比如 Q5 的性能已经远超预期），可以缩减投入，把精力放到更有价值的子问题上
- 如果发现新的关键假设（原来 5 个没覆盖的），可以增加实验
- H4.3 和 H4.4 可以合并为一个综合场景实验，如果发现分开做重复度太高

### 工具使用

- 编程语言不限于 Python（但 Python 优先，因为生态最丰富）
- 可以使用任何开源工具和库
- 可以使用商业 API（OpenAI、Anthropic 等）做 LLM 相关实验，注意记录成本

### 额外发现

- 如果实验过程中发现了架构文档没提到的问题或机会，鼓励记录下来
- 如果发现了更好的编码方案、更好的协商策略，可以提出建议

---

## 对接方式

### 提交位置

- 所有产出提交到：`research/H4_validation_experiments/`
- 目录结构建议：
  ```
  H4_validation_experiments/
  ├── H4.1_hypothesis_list.md          # 假设清单与优先级
  ├── H4.2_hdc_encoding/               # HDC 编码实验
  │   ├── encoding_experiment.ipynb
  │   ├── test_data/
  │   └── results/
  ├── H4.3_negotiation_vs_single/      # 协商 vs 单调用对比
  │   ├── experiment_design.md
  │   ├── results/
  │   └── evaluation_sheets/
  ├── H4.4_response_vs_search/         # 响应 vs 搜索范式对比
  │   ├── scenario_design.md
  │   ├── results/
  │   └── user_feedback/
  ├── H4.5_performance_benchmark/      # 端侧性能测试
  │   ├── benchmark.py
  │   └── results/
  └── H4.6_synthesis_report.md         # 综合分析报告
  ```

### 建议周期

- **H4.1**（假设清单）：1-2 天。这是轻量级工作，但质量很重要——后续所有实验都基于这个清单。
- **H4.2**（HDC 编码实验）：3-5 天。核心技术实验，需要仔细测量。
- **H4.3**（协商对比实验）：3-5 天。需要设计场景和做人工评估。
- **H4.4**（范式对比实验）：3-5 天。与 H4.3 有部分复用。
- **H4.5**（性能基准测试）：1-2 天。纯工程测量，依赖 H4.2 的编码产出。
- **H4.6**（综合报告）：2-3 天。需要消化所有实验结果。
- **总计建议周期**：2-3 周（H4.2/H4.3/H4.4 可并行推进）

### 并行策略

```
Week 1: H4.1 → H4.2（启动） + H4.3（启动）
Week 2: H4.2（完成） + H4.3（完成） + H4.4（启动） + H4.5（启动，依赖 H4.2 产出）
Week 3: H4.4（完成） + H4.5（完成） + H4.6（综合分析）
```

### 后续依赖

- **H4 的结论直接影响**：
  - V1 的技术选型确认（HDC 编码参数、协商轮次、k* 默认值）
  - A1（HDC 编码策略验证）的方向聚焦——H4 的实验是 A1 的快速验证前奏
  - H2（Prompt 工程研究）的优先级判断——如果协商效果好，Prompt 优化更值得投入
  - E3（端到端验证）的可行性判断——核心机制都验证过了，才值得跑完整流程
- **H4 的"失败"场景**：
  - 如果 HDC 编码效果不佳 → 为 A1 提供了明确的改进方向（换编码方法、调维度、换嵌入模型）
  - 如果协商效果不明显 → 检查是 Agent Profile 不够多样还是协商流程的问题
  - 如果响应范式没发现意外价值 → 检查场景设计是否有足够的"意外空间"，还是机制本身的局限
  - 无论哪种"失败"，实验数据本身都是永久资产

### 沟通

- 有架构理解问题 → 参考 `docs/ARCHITECTURE_DESIGN.md` 或使用 Arch Skill 讨论
- 有任务范围问题 → 参考本 PRD 或使用 Task Arch Skill 讨论
- 有代码实现问题 → 使用 Dev Skill 讨论

---

*本 PRD 由 Task Arch Skill 生成。实验的目的是量化和优化，不是质疑架构决策。*
*核心信念：好的架构不是在白板上完美设计出来的，而是在工程实践中验证和演化的。*
*关联文档：`docs/ARCHITECTURE_DESIGN.md`、`docs/DESIGN_LOG_003_PROJECTION_AS_FUNCTION.md`、`docs/CONTRIBUTION_TASK_CATALOG.md`*
