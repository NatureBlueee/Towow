# 模块二：结晶

## 通爻协议 · 模块二设计文档 · 自包含版

### 2026-02-16

---

## 一句话

模块二是一个有主持人的多人会议。场发现选出参与者，协议约束主持人的边界，会议的目标是消除与张力消解相关的信息差。

---

## 模块二在协议中的位置

通爻协议分两个模块。

模块一（意图场）解决：谁应该在同一个房间里。输入是全局张力场中的新扰动，输出是带关系类型标记的邻近图。这是一个数学问题——向量距离计算、关系类型识别（同向/互补/对冲）。零 LLM。V2 实验已验证，BGE-M3 编码器在多视角查询下达到 93% 准确率。模块一已完成。

模块二（结晶）解决：房间里的人如何发现彼此的价值。输入是模块一输出的邻近图，输出是参与方之间的协作构型，或者是残余张力（回到模块一触发新一轮）。这是一个理解问题——自然语言对话、模式识别、跨语义空间翻译。需要 LLM。本文档描述模块二的完整设计。

两个模块性质不同，工具不同。场发现用数学，结晶用对话。

---

## 核心机制

### 结构

模块二是一个循环。循环中有两种角色：

端侧 Agent（参与者）：代表自己的主人在讨论中发言。它了解主人但不了解其他参与者。它的能力由端侧实现决定——可能是 SecondMe 级别的深度个性化模型，可能是简单的 RAG 加通用 LLM。协议不假设端侧有任何特定能力。

催化 Agent（主持人）：同时听到所有参与者的发言，指出他们自己看不到的东西。它不了解任何参与者的个人信息，只看到本轮所有人的回复和之前的上下文。它是一个零私有信息的全局观察者。

每一轮循环：

1. 协议把当前上下文发给每个端侧 Agent
2. 每个端侧 Agent 用自然语言回复
3. 催化 Agent 读所有回复，用自然语言输出观察
4. 催化 Agent 的观察作为下一轮上下文的一部分，注入端侧 Agent 的输入

重复，直到催化 Agent 连续两轮没有新发现。收敛。

### 目的

消除与张力消解相关的信息差，用最少的轮次。

### 信息差的定义

信息差不是"A 不知道 B 的信息"。如果 B 的信息跟 A 的张力无关，A 不知道也无所谓。

信息差 = 与张力消解相关的信息，存在于系统中但没有到达需要它的人。

信息差是收敛的唯一障碍。如果所有参与方都能完美看到彼此的全部表达并且都有足够理解能力，一轮就收敛。现实中收敛慢，只因为两个原因：

信息没到达（传输问题）：A 说了一段话，B 没看到，或看到了但淹没在其他信息里。

信息到达了但没被理解（翻译问题）：A 用了 B 不理解的方式表达。A 说"我对分布式信任有直觉"，B 不知道这跟他的"需要一个能设计共识机制的人"是同一件事。

催化 Agent 同时解决这两个问题。

---

## 两个 prompt

整个模块二的工程实现归结为两个 prompt 的设计。一个给端侧 Agent，一个给催化 Agent。这两个 prompt 是协议哲学在工程层面的载体。

### 端侧 prompt

本质任务：在给定上下文下，产出一段对其他参与方有用的自我表达。

"有用"的最低标准：其他人读了之后能判断"跟我有关"还是"跟我无关"。更高标准是读了之后发现"原来我们可以这样合作"——但这是涌现的，不能被强制。

接口定义：

```
输入（协议提供）：
  - 触发上下文：什么事件把大家聚在一起的
  - 参与者信息：其他人的公开信息（来自模块一）
  - 上一轮观察：催化 Agent 的上一轮输出（首轮没有）

输出（端侧产出）：
  - 自然语言回复

约束（协议强制）：
  - 必须基于主人的真实情况
  - 必须与当前上下文相关
  - 无格式要求
```

设计原则：

不使用协议术语。端侧 Agent 不需要知道"结晶""张力""条件投影"是什么。它只需要理解"你在一个讨论里，代表你主人说话"。协议的概念是我们的思维脚手架，不是端侧的操作指令。

不强制输出格式。如果主人想表达的东西不可拆分——比如"我对这个方向有一种说不清的共鸣"——强制结构化会杀死信息中最有价值的部分。自然语言是 Agent 最擅长的输出形式，保真度最高。

不限制表达维度。"你能贡献什么/你需要什么/你不能接受什么"适用于大多数场景，但最有价值的回复可能是"你说的让我想到一件事"——既不是贡献也不是需求，但可能是结晶的催化点。Prompt 引导方向但不封闭维度。

协议提供参考 prompt 模板（默认实现），端侧可以完全自定义。

### 催化 prompt

本质任务：读 N 个人的自然语言回复，指出他们自己看不到的东西。

核心策略：消除信息差。具体分为两个子任务。

子任务一：传输——确保与张力消解相关的信息到达需要它的人。

催化 Agent 同时听到了所有人。它能看到 A 的盈余恰好对应 B 的亏缺，但 A 和 B 各自看不到对方。催化 Agent 指出这个连接。不推荐他们合作，只让他们看到彼此。

优先级排序（按对收敛的影响力）：

1. 未被发现的匹配——A 能满足 B，B 能满足 A，但双方没意识到。消除这个信息差直接产生张力消解。
2. 隐藏的阻碍——A 和 B 看起来匹配，但 A 有一个未说出的约束跟 B 的假设矛盾。不现在指出会导致假收敛。
3. 不存在的角色——所有人的张力叠加后某个维度没人能覆盖。越早知道越早决定是外部补充还是接受残余。
4. 被忽略的信息——某人提了一个重要约束但没有人回应。
5. 重复的供给——多个参与方在提供同一种能力，意识到重复后可能暴露被遮盖的其他维度。

子任务二：翻译——让到达的信息被理解。

现实的多 Agent 场景中，参与方的语料库不同、RAG 数据库不同、表达习惯不同。一个诗人的 Agent 说"我在寻找能把虚空编织成形的人"，一个工程师的 Agent 说"我擅长从模糊需求中构建系统架构"。这两个表达之间有深度互补，但双方的 Agent 大概率不会识别到。

催化 Agent 同时听到了两段话。它指出："A 描述的'把虚空编织成形'和 B 的'从模糊需求中构建系统架构'可能在说同一种能力。" 不是替人决定他们匹配，是让人意识到他们可能在说同一件事。

跨语义空间的模式识别恰好是 LLM 最擅长的能力。这是催化 Agent 必须用 LLM 的根本原因。

接口定义：

```
输入（协议提供）：
  - 触发上下文
  - 本轮所有端侧 Agent 的回复
  - 历史上下文（前几轮的催化输出）

输出（催化 Agent 产出）：
  - 自然语言观察

约束（协议强制）：
  - 不推荐行动方案
  - 不评价任何参与方的质量或价值
  - 不遗漏任何参与方（每个人的回复都必须被看到）
  - 输出指向关系（人与人之间），不指向个体评判
```

催化 Agent 可自定义。协议提供默认催化策略，但用户可以上传自己的催化偏好。创意项目可能希望催化更关注"意外联想"，工程外包可能希望严格关注"技能匹配"。催化 Agent 是用户意图的延伸——"我希望这次会议中什么被看到"——只是带有协议的约束边界。

约束是协议层的（稳定、不可违反），策略是应用层的（可迭代、可个性化）。

---

## 收敛

收敛 = 催化 Agent 连续两轮没有新发现。

所有与张力消解相关的信息都已到达需要它的人，且已被理解。再多聊一轮不会有新东西。这是信息差枯竭的自然信号。

不收敛时：设置最大轮次。达到上限后将当前状态呈现给触发方（人），由人决定接受、放弃或补充参与者。

### 残余与递归

结晶完成后，参与方的张力叠加仍有未消解部分 = 残余。残余本身成为新的张力粒子，回到模块一的场中，触发新一轮场发现。

这不是设计出来的"子需求功能"。是同一个操作的自然重复。残余和原始张力在协议层面完全同构。

---

## 关键约束与设计决策

### 催化 Agent 为什么不能推荐方案？

催化 Agent 的每一句输出都会被端侧 Agent 消费并影响下一轮表达。如果它说"A 和 B 应该合作"，A 和 B 可能只因为催化 Agent 说了这句话就去合作——即使他们自己还没确认。催化 Agent 的话有隐性权力。这种权力必须被限制在"让人看见"而不是"替人选择"。

这是协议的核心原则——协议止于可见性——在模块二中的具体体现。

### 为什么用自然语言而不是结构化向量？

三个理由。Agent 最擅长自然语言输出，强制结构化是在最弱的地方要求输出。催化 Agent 也是 LLM，读自然语言识别模式恰好是 LLM 最擅长的事。结晶阶段的目标是发现协作可能，不是精确数值匹配，自然语言携带的上下文、语气、边界感信息量更大。

场发现用向量（数学问题），结晶用自然语言（理解问题）。两个模块性质不同，工具不同。

### 端侧 Agent 能力差怎么办？

协议不管。你的 Agent 质量差，你在网络中的协作效率就低，这是自然淘汰。协议提供参考 prompt 模板帮助大多数人获得合理的输出质量，但不强制。

### 催化 Agent 用什么模型？

用最好的。催化 Agent 的智能程度直接决定信息差识别和跨语义翻译的质量。这是系统中最值得投入算力的位置——一次成功的结晶带来的协作价值远大于几十块钱的算力成本。

初期平台承担成本。后续可以由发起方选择模型等级和承担成本，或参与方分摊。技术上就是换一个 API key。

---

## 两个深化方向

模块二的设计框架已经闭合。接下来的深化集中在两个方向，两个方向都归结为 prompt 工程。

### 深化方向一：翻译

怎么让催化 Agent 更好地识别跨语义空间的等价表达？

这是一个 prompt 设计问题。催化 Agent 需要被引导去做的事：当两个人用不同的词汇空间描述相似的事物时，指出相似性。不是替人确认"你们在说同一件事"，而是"你们可能在说同一件事"。

翻译的难度随参与者的异质性增加。两个工程师之间信息差小，一个诗人和一个工程师之间信息差大。催化 Agent 的翻译能力是系统处理高异质性协作的上限。

具体实现：催化 prompt 中如何引导 LLM 做跨语义空间的模式识别。这需要实验——不同的引导方式在不同场景下效果不同。需要测试用例和评估框架。

### 深化方向二：传输策略

怎么让催化 Agent 在每一轮选择最有效的信息传递？

这也是一个 prompt 设计问题。催化 Agent 每轮的输出长度有限，不可能传达所有关系。它需要优先传达对收敛影响最大的信息差。上面列出的五级优先级是初始框架，需要在实际场景中验证和迭代。

还有一个策略维度：催化 Agent 在早期轮次应该保持广度（不过早聚焦于单一匹配），在后期轮次可以聚焦深度（引导特定的参与者之间深入交流）。催化 Agent 甚至可以建议两个特定 Agent 单独对话——这是它的工具之一，不需要预定义，它可以根据当前信息差的分布自己判断。

---

## 从推导链中沉淀的不可逆认知

模块二的设计经历了多轮概念膨胀和修剪。每一轮都留下了不可逆的认知：

博弈论 → 变分原理。Agent 之间不存在策略行为。系统滑向最小势能。这杀死了 V1 Center 的"综合方案"功能。

五步流水线 → 两步一循环。Formulation 不是预处理，是循环中自然发生的。人看到上下文后才知道自己要什么。

结构化向量 → 自然语言。场发现用数学，结晶用理解。性质不同，工具不同。

条件投影三子步骤 → 端侧 Agent 的一次回复。协议不规定端侧怎么思考。SecondMe 已经解了个性化记忆问题。协议给上下文，Agent 给回复，就这样。

拓扑分析器 → 圆桌主持人。催化 Agent 做的就是主持会议。所有概念最终坍缩为一个人人能理解的比喻。

### 被砍掉的概念

相变、直接键合、催化结晶、构型呈现、兑现探测、二阶结构、收敛锚定、谢林点、provides/requires、边界条件、拓扑分析器……

每个被砍掉的概念都曾经看起来有用。它们的价值在于推导过程中加速了思维——作为脚手架。但脚手架不是建筑。最终留下的是：一个有主持人的圆桌会议，主持人的工作是消除信息差。

### 最终保留的六个概念

1. 张力（唯一粒子）
2. 三种关系类型：同向、互补、对冲（逻辑完备）
3. 投影（唯一操作）
4. 条件投影 = 端侧 Agent 在特定上下文下的一次回复
5. 催化 Agent = 有协议约束的圆桌主持人
6. 残余张力 = 递归的驱动力

六个概念描述整个协议。

---

## V1 → V2 映射

| V1 | V2 | 变化性质 |
|---|---|---|
| Formulate（需求丰富化）| 吸收进循环（每轮自然发生）| 从预处理变为持续过程 |
| Resonate（共振匹配）| 模块一：场发现 | 从 demand-only 扩展为任意张力触发 |
| Offer（Agent 回应）| 端侧 Agent 的第一轮回复 | 从"对需求的响应"变为"自我表达" |
| Center 综合 | 催化 Agent 的观察 | 从输出方案降级为指出关系 |
| Plan（方案输出）| 收敛时的状态 | 不再被生成，自然涌现 |
| Approve（用户确认）| 端侧自然行为 | 从协议步骤变为协议外的自主决策 |
| 子需求递归 | 残余张力回到场中 | 从特殊功能变为同一操作的自然重复 |

---

## LLM 使用边界

| 场景 | LLM | 原因 |
|---|---|---|
| 模块一：场发现 | 否 | 向量距离计算，纯数学 |
| 端侧 Agent 回复 | 是 | 基于个人记忆的情境化表达 |
| 催化 Agent 观察 | 是 | 跨表达的模式识别和翻译 |
| 收敛检测 | 由催化 Agent 自行判断 | "我这轮有没有新发现" |
| 残余计算 | 视张力性质而定 | 可拆分用代码，不可拆分用 LLM |

---

*模块二就是这么简单。一个有主持人的多人会议。主持人消除信息差。参与者越来越清楚彼此的价值。直到没有新东西可说。*
