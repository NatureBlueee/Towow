# S4 — SDK 集成测试与压力测试

> 创建日期：2026-02-09
> 任务类型：SDK 工程质量 × 性能验证
> 优先级：Tier 2（重要但不紧急，V1 已有 190 测试覆盖核心逻辑）
> PRD 状态：已细化
> 依赖：建议 S1 先完成（发现 SDK 的实际使用模式后再做压力测试更有针对性）
> 关联任务：H4（最小验证实验，H4 验证架构假设，S4 验证工程质量）、S3（Adapter 实现，可用于多 LLM 压力测试）

---

## 这个任务在项目中的位置

V1 引擎已有 190 个单元/集成测试，覆盖了核心状态机、Skill 调用、事件推送、确认机制等。但这些测试都是"正常路径"——3-5 个 Agent、1 次协商、mock LLM。

**S4 回答的是**：当 N 个 Agent × M 次并发协商同时运行时，SDK 的表现如何？瓶颈在哪？什么条件下会崩溃？

```
[V1 测试: 190 tests] → 正确性验证（小规模、单协商）
[S4: 压力测试（你在这里）] → 性能和稳定性验证（大规模、并发）
                              ├── N Agent 规模下的内存和速度
                              ├── M 并发协商的资源竞争
                              └── 异常场景下的恢复能力
```

**与 H4 的区别**：H4 验证的是"架构假设是否成立"（HDC 编码质量、协商 vs 单调用的方案质量），S4 验证的是"工程实现是否扛得住"（并发、内存、超时、错误恢复）。

**与 V1/V2 解耦**：只使用 SDK 公开 API 和 EngineBuilder 构造引擎，不修改核心代码。

---

## 为什么做这件事

SDK 的工程质量问题在小规模测试中看不出来——3 个 Agent 的协商，内存泄漏无所谓、锁竞争不明显、超时处理不触发。但真实使用场景中：

1. **黑客松组队**：50-100 个参赛者同时在线，可能有 10+ 个组队需求并发
2. **技能交换**：几百个自由职业者的 Profile 同时被检索和协商
3. **LLM 调用**：每次协商至少 3-5 次 LLM 调用，并发 10 个协商就是 30-50 次并发 LLM 调用

不做压力测试就上线，等于蒙着眼睛开快车。

---

## 你要回答什么问题

**核心问题**：通爻 SDK 在真实负载下的性能瓶颈在哪？极限在哪？

**子问题**：

1. **单协商性能基线**：一次完整的协商（5 Agent）从提交需求到输出方案，端到端耗时多少？各阶段（formulation / offer / center）各占多少时间？
2. **Agent 规模扩展**：从 5 到 10 到 50 到 100 个 Agent，协商耗时和内存如何增长？增长是线性还是超线性？
3. **并发协商**：同时运行 2 / 5 / 10 / 20 个独立协商，资源消耗如何？是否有锁竞争或状态混淆？
4. **LLM 调用瓶颈**：LLM API 调用是整个流程中最慢的环节。当并发 LLM 调用数量增加时，rate limiting 和超时如何影响整体？
5. **异常恢复**：LLM 调用超时、连接断开、返回格式错误时，SDK 的表现如何？会崩溃还是优雅降级？
6. **内存泄漏**：长时间运行（100+ 次协商）后，内存占用是否持续增长？

---

## 我们提供什么

### SDK 和测试基础

| 资源 | 位置 | 用途 |
|------|------|------|
| SDK 开发者指南 | `backend/docs/SDK_GUIDE.md` | EngineBuilder 使用方法 |
| 现有测试 | `backend/tests/towow/` | 190 个测试的模式和 mock 可参考 |
| conftest.py | `backend/tests/towow/conftest.py` | MockPlatformLLMClient 可直接复用 |
| 内置 Adapter | `backend/towow/adapters/` | 真实 LLM 集成测试可用 |

### V1 引擎内部结构（理解瓶颈用）

| 组件 | 位置 | 性能相关 |
|------|------|---------|
| Engine | `backend/towow/core/engine.py` | 状态机 + asyncio 编排，所有协商共享同一个 Engine 实例 |
| EventPusher | `backend/towow/infra/event_pusher.py` | WebSocket 推送，高并发时可能是瓶颈 |
| LLM Client | `backend/towow/infra/llm_client.py` | 所有 LLM 调用的入口，httpx 客户端 |

### 已知的性能特征

- 每次协商约 3-7 次 LLM 调用（formulation 1 + offer N + center 1-2 + 可能的 sub_negotiation）
- V1 确认机制使用 `asyncio.Event`，有 300 秒超时
- Center 的 LLM 调用有 2 轮上限（代码保障）
- 子协商最大深度为 1（代码保障）

---

## 子任务分解

### S4.1 — 性能基线测量

**描述**：测量单次协商的端到端性能基线。使用 mock LLM（固定延迟 + 固定响应）消除 LLM 变量。

**依赖**：无

**交付物**：
- 基线测量脚本（可复用）
- 单次协商各阶段耗时分解（formulation / offer_generation × N / center_round_1 / center_round_2）
- 内存占用基线（协商前 vs 协商后）
- Agent 数量（5/10/20/50/100）× 协商耗时/内存 矩阵

### S4.2 — 并发协商测试

**描述**：同时启动多个独立的协商，测量资源竞争和状态隔离。

**依赖**：S4.1（需要基线数据做对比）

**交付物**：
- 并发测试脚本
- 并发数（1/2/5/10/20）× 总吞吐量/单协商耗时/内存占用 矩阵
- 状态隔离验证：并发协商的结果是否独立？有没有数据串扰？
- 资源竞争分析：CPU/内存/asyncio event loop 的利用率

### S4.3 — LLM 瓶颈分析

**描述**：针对 LLM 调用这个最慢的环节，专门测试不同 LLM 配置下的性能。

**依赖**：S4.1

**交付物**：
- 测试矩阵：
  - Mock LLM（固定延迟 50ms / 200ms / 1s / 5s）× 并发协商数
  - 真实 LLM（如有 API Key）× 并发协商数
- LLM rate limiting 对整体性能的影响
- 超时策略评估：当前 300 秒确认超时是否合理？
- 建议：LLM 调用的最优并发策略

### S4.4 — 异常场景测试

**描述**：模拟各种异常情况，验证 SDK 的健壮性。

**依赖**：S4.1

**交付物**：
- 异常场景清单（每个场景：触发方式 + 预期行为 + 实际行为）：
  - LLM 调用超时
  - LLM 返回空响应
  - LLM 返回格式错误（非 JSON）
  - 协商中途 Agent 数据源不可用
  - 确认超时（300 秒无响应）
  - 内存压力下的表现
- 每个异常场景的 SDK 实际表现记录
- Bug 列表（如果有）
- 健壮性改进建议

### S4.5 — 综合性能报告

**描述**：汇总 S4.1-S4.4 的测试结果，分析瓶颈，给出优化建议。

**依赖**：S4.1 + S4.2 + S4.3 + S4.4

**交付物**：
- **性能报告**（3000-4000 字）：
  - 性能基线摘要（单协商 / 多 Agent / 并发）
  - 瓶颈识别（排名前 3 的性能瓶颈）
  - LLM 调用策略建议
  - 规模上限估算（当前架构能支撑多大规模？）
  - 优化建议（短期 workaround + 长期架构改进）
  - 与 V1 规模目标（1000 Agent、100 消息/秒）的差距分析

---

## 做完了是什么样

### 产出清单

1. **可复用的性能测试套件**（脚本 + 配置）
2. **性能矩阵数据**（Agent 规模 × 并发数 × 各指标）
3. **异常场景测试报告**
4. **综合性能报告 + 优化建议**

### 三级质量标准

**做完了（基本合格）**：
- 有单协商性能基线
- 有并发测试数据
- 瓶颈识别有数据支撑
- 测试脚本可复用

**做得好（超出预期）**：
- 发现了至少 1 个非显然的性能问题
- 优化建议具体到代码级别（"engine.py 第 X 行的 await 可以改为并发"）
- 测试套件被纳入 CI/CD 流程

**做得出色（产生额外价值）**：
- 发现并修复了一个实际的 Bug（内存泄漏、状态串扰等）
- 性能报告的数据足以写入对外材料
- 性能测试框架成为 SDK 的标准质量保障工具

---

## 你必须遵守的

1. **使用 SDK 公开 API** 构造测试场景（EngineBuilder + Protocol 实现）
2. **所有测试必须可复现**——记录硬件环境、Python 版本、依赖版本
3. **区分 mock 测试和真实 LLM 测试**——不要把 mock 数据当真实性能报告
4. **不修改核心代码**——如果发现需要改才能优化，在报告中写"架构建议"

---

## 你可以自己决定的

- 测试工具选择（pytest-benchmark / locust / 自写脚本）
- 测试环境（本地 / 云服务器）
- Mock LLM 的延迟设置
- 异常场景的优先级排序
- 是否做内存 profiling（推荐但非必须）

---

## 对接方式

### 提交位置
- 测试代码：`research/S4_stress_test/`
- 性能报告：`research/S4_stress_test/performance_report.md`

### 建议周期
- S4.1（基线）：2 天
- S4.2（并发）：2 天
- S4.3（LLM 瓶颈）：2 天
- S4.4（异常）：2 天
- S4.5（报告）：2 天
- 总计：1.5-2 周

### 后续依赖
- S4 的性能数据直接影响 V2 的架构决策
- S4 发现的 Bug 进入修复队列
- 性能测试套件成为持续质量保障的一部分

---

*本 PRD 于 2026-02-09 任务审查中创建。S 系列任务与 V1/V2 核心开发完全解耦。*
*参考文档：`backend/docs/SDK_GUIDE.md`、`backend/tests/towow/conftest.py`*
